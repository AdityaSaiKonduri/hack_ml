{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.tsv\",sep='\\t')\n",
    "test_df = pd.read_csv(\"test.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = pd.read_csv(\"train_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tit_len = pd.read_csv(\"train_title_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_length = pd.read_csv(\"train_text_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_train_outputs_text = pd.read_csv(\"vader_train_outputs_text.csv\")\n",
    "vader_train_outputs_title = pd.read_csv(\"vader_train_outputs_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuated_train_df = pd.read_csv('punctuate_train.csv')\n",
    "punctuated_test_df = pd.read_csv('punctuate_test.csv')\n",
    "punctuated_test_df = punctuated_test_df['Punctuate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_train_title = pd.read_csv('title_keyword_density.tsv', sep = '\\t')\n",
    "keyword_train_text = pd.read_csv('text_keyword_density.tsv', sep = '\\t')\n",
    "\n",
    "keyword_test_title = pd.read_csv('test_title_keyword_density.tsv', sep = '\\t')\n",
    "keyword_test_text = pd.read_csv('test_text_keyword_density.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>2733</td>\n",
       "      <td>257</td>\n",
       "      <td>16</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.536779</td>\n",
       "      <td>1.988072</td>\n",
       "      <td>2.584493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "      <td>2630</td>\n",
       "      <td>271</td>\n",
       "      <td>14</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.9197</td>\n",
       "      <td>0</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.384458</td>\n",
       "      <td>2.862986</td>\n",
       "      <td>2.658487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>4052</td>\n",
       "      <td>404</td>\n",
       "      <td>13</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.311902</td>\n",
       "      <td>1.778386</td>\n",
       "      <td>2.599179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>1131</td>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.9335</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.615385</td>\n",
       "      <td>2.403846</td>\n",
       "      <td>1.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>1061</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.4559</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>11.475410</td>\n",
       "      <td>2.732240</td>\n",
       "      <td>2.185792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0            1                67                 8             2733   \n",
       "1            2               121                14             2630   \n",
       "2            1                64                 7             4052   \n",
       "3            3                72                 7             1131   \n",
       "4            4               104                10             1061   \n",
       "\n",
       "   text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0              257                   16  0.109  0.789  0.102   -0.5574   \n",
       "1              271                   14  0.095  0.832  0.073   -0.9197   \n",
       "2              404                   13  0.052  0.859  0.089    0.9826   \n",
       "3              107                    5  0.022  0.884  0.094    0.9335   \n",
       "4              100                    7  0.077  0.836  0.087    0.4559   \n",
       "\n",
       "   Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0          1   10.000000          0.0    0.000000  10.536779  1.988072   \n",
       "1          0    4.347826          0.0    0.000000   8.384458  2.862986   \n",
       "2          1   16.666667          0.0    0.000000  12.311902  1.778386   \n",
       "3          0   20.000000          0.0    0.000000   9.615385  2.403846   \n",
       "4          1    6.666667          0.0    6.666667  11.475410  2.732240   \n",
       "\n",
       "    RB_text  \n",
       "0  2.584493  \n",
       "1  2.658487  \n",
       "2  2.599179  \n",
       "3  1.923077  \n",
       "4  2.185792  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([train_cat, train_tit_len, train_text_length, \n",
    "                       vader_train_outputs_text, vader_train_outputs_title, punctuated_train_df, keyword_train_title, keyword_train_text], axis=1)\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "merged_df.drop(columns=[\"id\", \"Unnamed: 0\"],inplace=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "29995    1\n",
       "29996    1\n",
       "29997    0\n",
       "29998    1\n",
       "29999    1\n",
       "Name: label, Length: 30000, dtype: int64"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "print(len(train_df['label']), len(merged_df))\n",
    "train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(merged_df, train_df['label'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>1356</td>\n",
       "      <td>127</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1930</td>\n",
       "      <td>180</td>\n",
       "      <td>15</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.205479</td>\n",
       "      <td>3.013699</td>\n",
       "      <td>1.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>1526</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>1</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>5.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "      <td>6187</td>\n",
       "      <td>573</td>\n",
       "      <td>36</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.9899</td>\n",
       "      <td>1</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.523246</td>\n",
       "      <td>1.521555</td>\n",
       "      <td>4.564666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>2517</td>\n",
       "      <td>235</td>\n",
       "      <td>19</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.720978</td>\n",
       "      <td>3.054990</td>\n",
       "      <td>3.665988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "      <td>658</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.8308</td>\n",
       "      <td>1</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.727273</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>4.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>2156</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>1</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.706468</td>\n",
       "      <td>1.741294</td>\n",
       "      <td>1.741294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>4</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>1351</td>\n",
       "      <td>134</td>\n",
       "      <td>10</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>1</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.822511</td>\n",
       "      <td>2.164502</td>\n",
       "      <td>2.164502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>5376</td>\n",
       "      <td>502</td>\n",
       "      <td>32</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.4400</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.578947</td>\n",
       "      <td>2.315789</td>\n",
       "      <td>3.684211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0                3                75                11             1356   \n",
       "1                5                55                 7             1930   \n",
       "2                1                66                 8             1526   \n",
       "3                6                86                 8             6187   \n",
       "4                6                75                 8             2517   \n",
       "...            ...               ...               ...              ...   \n",
       "23995            8                80                14              658   \n",
       "23996            5                67                 8             2156   \n",
       "23997            4               145                 1              145   \n",
       "23998            5                48                 7             1351   \n",
       "23999            1                72                 9             5376   \n",
       "\n",
       "       text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0                  127                    8  0.044  0.892  0.064    0.5411   \n",
       "1                  180                   15  0.135  0.856  0.009   -0.9890   \n",
       "2                  150                    8  0.145  0.708  0.147   -0.4767   \n",
       "3                  573                   36  0.114  0.800  0.086   -0.9899   \n",
       "4                  235                   19  0.073  0.840  0.086    0.5282   \n",
       "...                ...                  ...    ...    ...    ...       ...   \n",
       "23995               69                    4  0.084  0.916  0.000   -0.8308   \n",
       "23996              200                   12  0.042  0.876  0.082    0.9274   \n",
       "23997                1                    1  0.000  1.000  0.000    0.0000   \n",
       "23998              134                   10  0.050  0.877  0.073    0.6360   \n",
       "23999              502                   32  0.072  0.853  0.074   -0.4400   \n",
       "\n",
       "       Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0              0    5.555556         0.00         0.0   9.523810  0.000000   \n",
       "1              1    0.000000         0.00         0.0   5.205479  3.013699   \n",
       "2              1   40.000000         0.00         0.0  13.888889  0.694444   \n",
       "3              1   12.500000         0.00        12.5   7.523246  1.521555   \n",
       "4              1    0.000000         0.00         0.0   6.720978  3.054990   \n",
       "...          ...         ...          ...         ...        ...       ...   \n",
       "23995          1   18.750000         6.25         0.0  12.727273  4.545455   \n",
       "23996          1   40.000000         0.00         0.0   8.706468  1.741294   \n",
       "23997          1   33.333333         0.00         0.0  33.333333  0.000000   \n",
       "23998          1   11.111111         0.00         0.0  10.822511  2.164502   \n",
       "23999          1   33.333333         0.00         0.0   9.578947  2.315789   \n",
       "\n",
       "        RB_text  \n",
       "0      1.298701  \n",
       "1      1.917808  \n",
       "2      5.208333  \n",
       "3      4.564666  \n",
       "4      3.665988  \n",
       "...         ...  \n",
       "23995  4.545455  \n",
       "23996  1.741294  \n",
       "23997  0.000000  \n",
       "23998  2.164502  \n",
       "23999  3.684211  \n",
       "\n",
       "[24000 rows x 17 columns]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype = torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNetwork(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "\n",
    "        # 1D Convolutional layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),  # (16, 17)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (16, 8)\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1),  # (32, 8)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (32, 4)\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),  # (64, 4)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)  # (64, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after the convolutional layers\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 128),  # Flattened input to the fully connected layer (64 channels, 2 length)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # Output layer with 2 units for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 17)  # Reshape input to [batch_size, 1, 17]\n",
    "        x = self.conv_layer(x)  # Apply convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor before feeding it into the fully connected layers\n",
    "        x = self.fc_layer(x)  # Apply fully connected layers\n",
    "        return x\n",
    "    \n",
    "model = ConvolutionalNetwork().to(device)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.677827  [   32/24000]\n",
      "loss: 0.575739  [ 3232/24000]\n",
      "loss: 0.527260  [ 6432/24000]\n",
      "loss: 0.464972  [ 9632/24000]\n",
      "loss: 0.548976  [12832/24000]\n",
      "loss: 0.502925  [16032/24000]\n",
      "loss: 0.489505  [19232/24000]\n",
      "loss: 0.463742  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.454500 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.354732  [   32/24000]\n",
      "loss: 0.514523  [ 3232/24000]\n",
      "loss: 0.355593  [ 6432/24000]\n",
      "loss: 0.399638  [ 9632/24000]\n",
      "loss: 0.267157  [12832/24000]\n",
      "loss: 0.415693  [16032/24000]\n",
      "loss: 0.310008  [19232/24000]\n",
      "loss: 0.321101  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.378731 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.258850  [   32/24000]\n",
      "loss: 0.306196  [ 3232/24000]\n",
      "loss: 0.424746  [ 6432/24000]\n",
      "loss: 0.484522  [ 9632/24000]\n",
      "loss: 0.340423  [12832/24000]\n",
      "loss: 0.367432  [16032/24000]\n",
      "loss: 0.330844  [19232/24000]\n",
      "loss: 0.401697  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.356075 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.334882  [   32/24000]\n",
      "loss: 0.323747  [ 3232/24000]\n",
      "loss: 0.371564  [ 6432/24000]\n",
      "loss: 0.429016  [ 9632/24000]\n",
      "loss: 0.302517  [12832/24000]\n",
      "loss: 0.348008  [16032/24000]\n",
      "loss: 0.260937  [19232/24000]\n",
      "loss: 0.353531  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.343978 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.213855  [   32/24000]\n",
      "loss: 0.366181  [ 3232/24000]\n",
      "loss: 0.461380  [ 6432/24000]\n",
      "loss: 0.568904  [ 9632/24000]\n",
      "loss: 0.374051  [12832/24000]\n",
      "loss: 0.239749  [16032/24000]\n",
      "loss: 0.315679  [19232/24000]\n",
      "loss: 0.336718  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.353788 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.395111  [   32/24000]\n",
      "loss: 0.417903  [ 3232/24000]\n",
      "loss: 0.220632  [ 6432/24000]\n",
      "loss: 0.275726  [ 9632/24000]\n",
      "loss: 0.345482  [12832/24000]\n",
      "loss: 0.386575  [16032/24000]\n",
      "loss: 0.277167  [19232/24000]\n",
      "loss: 0.271177  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.337981 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.394865  [   32/24000]\n",
      "loss: 0.305936  [ 3232/24000]\n",
      "loss: 0.370056  [ 6432/24000]\n",
      "loss: 0.420734  [ 9632/24000]\n",
      "loss: 0.408590  [12832/24000]\n",
      "loss: 0.288875  [16032/24000]\n",
      "loss: 0.221820  [19232/24000]\n",
      "loss: 0.474550  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.343441 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.273023  [   32/24000]\n",
      "loss: 0.204624  [ 3232/24000]\n",
      "loss: 0.545390  [ 6432/24000]\n",
      "loss: 0.255422  [ 9632/24000]\n",
      "loss: 0.235552  [12832/24000]\n",
      "loss: 0.214195  [16032/24000]\n",
      "loss: 0.238286  [19232/24000]\n",
      "loss: 0.160346  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.323260 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.247457  [   32/24000]\n",
      "loss: 0.352601  [ 3232/24000]\n",
      "loss: 0.220972  [ 6432/24000]\n",
      "loss: 0.456251  [ 9632/24000]\n",
      "loss: 0.411646  [12832/24000]\n",
      "loss: 0.323083  [16032/24000]\n",
      "loss: 0.228296  [19232/24000]\n",
      "loss: 0.508327  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.319294 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.321064  [   32/24000]\n",
      "loss: 0.300243  [ 3232/24000]\n",
      "loss: 0.400274  [ 6432/24000]\n",
      "loss: 0.410965  [ 9632/24000]\n",
      "loss: 0.440382  [12832/24000]\n",
      "loss: 0.318878  [16032/24000]\n",
      "loss: 0.230172  [19232/24000]\n",
      "loss: 0.422972  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.311402 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.315119  [   32/24000]\n",
      "loss: 0.256778  [ 3232/24000]\n",
      "loss: 0.228655  [ 6432/24000]\n",
      "loss: 0.321362  [ 9632/24000]\n",
      "loss: 0.315572  [12832/24000]\n",
      "loss: 0.430816  [16032/24000]\n",
      "loss: 0.174632  [19232/24000]\n",
      "loss: 0.235909  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.309859 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.223474  [   32/24000]\n",
      "loss: 0.335210  [ 3232/24000]\n",
      "loss: 0.551899  [ 6432/24000]\n",
      "loss: 0.386359  [ 9632/24000]\n",
      "loss: 0.292406  [12832/24000]\n",
      "loss: 0.529487  [16032/24000]\n",
      "loss: 0.318505  [19232/24000]\n",
      "loss: 0.370837  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.318734 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.463308  [   32/24000]\n",
      "loss: 0.301797  [ 3232/24000]\n",
      "loss: 0.295583  [ 6432/24000]\n",
      "loss: 0.172705  [ 9632/24000]\n",
      "loss: 0.229507  [12832/24000]\n",
      "loss: 0.483736  [16032/24000]\n",
      "loss: 0.341888  [19232/24000]\n",
      "loss: 0.345027  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.307451 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.396738  [   32/24000]\n",
      "loss: 0.249903  [ 3232/24000]\n",
      "loss: 0.274712  [ 6432/24000]\n",
      "loss: 0.215376  [ 9632/24000]\n",
      "loss: 0.456188  [12832/24000]\n",
      "loss: 0.390869  [16032/24000]\n",
      "loss: 0.362706  [19232/24000]\n",
      "loss: 0.555529  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.302781 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.395755  [   32/24000]\n",
      "loss: 0.519253  [ 3232/24000]\n",
      "loss: 0.280280  [ 6432/24000]\n",
      "loss: 0.364562  [ 9632/24000]\n",
      "loss: 0.236652  [12832/24000]\n",
      "loss: 0.284377  [16032/24000]\n",
      "loss: 0.399140  [19232/24000]\n",
      "loss: 0.161339  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.299535 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.250414  [   32/24000]\n",
      "loss: 0.326104  [ 3232/24000]\n",
      "loss: 0.332199  [ 6432/24000]\n",
      "loss: 0.201074  [ 9632/24000]\n",
      "loss: 0.353244  [12832/24000]\n",
      "loss: 0.325002  [16032/24000]\n",
      "loss: 0.308742  [19232/24000]\n",
      "loss: 0.312143  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.306923 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.347133  [   32/24000]\n",
      "loss: 0.445092  [ 3232/24000]\n",
      "loss: 0.449048  [ 6432/24000]\n",
      "loss: 0.357654  [ 9632/24000]\n",
      "loss: 0.278730  [12832/24000]\n",
      "loss: 0.329259  [16032/24000]\n",
      "loss: 0.302211  [19232/24000]\n",
      "loss: 0.208468  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.297288 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.224703  [   32/24000]\n",
      "loss: 0.275431  [ 3232/24000]\n",
      "loss: 0.292835  [ 6432/24000]\n",
      "loss: 0.462308  [ 9632/24000]\n",
      "loss: 0.227627  [12832/24000]\n",
      "loss: 0.341714  [16032/24000]\n",
      "loss: 0.239332  [19232/24000]\n",
      "loss: 0.296840  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.294940 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.531170  [   32/24000]\n",
      "loss: 0.227250  [ 3232/24000]\n",
      "loss: 0.357472  [ 6432/24000]\n",
      "loss: 0.269540  [ 9632/24000]\n",
      "loss: 0.303096  [12832/24000]\n",
      "loss: 0.285781  [16032/24000]\n",
      "loss: 0.242364  [19232/24000]\n",
      "loss: 0.291978  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.293843 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.431587  [   32/24000]\n",
      "loss: 0.142831  [ 3232/24000]\n",
      "loss: 0.128836  [ 6432/24000]\n",
      "loss: 0.236178  [ 9632/24000]\n",
      "loss: 0.209588  [12832/24000]\n",
      "loss: 0.264232  [16032/24000]\n",
      "loss: 0.223621  [19232/24000]\n",
      "loss: 0.350524  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.296554 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.273587  [   32/24000]\n",
      "loss: 0.306113  [ 3232/24000]\n",
      "loss: 0.259203  [ 6432/24000]\n",
      "loss: 0.287770  [ 9632/24000]\n",
      "loss: 0.190770  [12832/24000]\n",
      "loss: 0.148333  [16032/24000]\n",
      "loss: 0.218707  [19232/24000]\n",
      "loss: 0.369975  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.288030 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.255818  [   32/24000]\n",
      "loss: 0.313371  [ 3232/24000]\n",
      "loss: 0.425474  [ 6432/24000]\n",
      "loss: 0.249126  [ 9632/24000]\n",
      "loss: 0.112753  [12832/24000]\n",
      "loss: 0.269155  [16032/24000]\n",
      "loss: 0.238689  [19232/24000]\n",
      "loss: 0.251093  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.287166 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.242241  [   32/24000]\n",
      "loss: 0.212664  [ 3232/24000]\n",
      "loss: 0.191478  [ 6432/24000]\n",
      "loss: 0.353496  [ 9632/24000]\n",
      "loss: 0.260655  [12832/24000]\n",
      "loss: 0.255078  [16032/24000]\n",
      "loss: 0.268229  [19232/24000]\n",
      "loss: 0.228515  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.288292 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.213018  [   32/24000]\n",
      "loss: 0.174607  [ 3232/24000]\n",
      "loss: 0.184352  [ 6432/24000]\n",
      "loss: 0.401491  [ 9632/24000]\n",
      "loss: 0.257540  [12832/24000]\n",
      "loss: 0.218443  [16032/24000]\n",
      "loss: 0.199936  [19232/24000]\n",
      "loss: 0.332406  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.283802 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.487346  [   32/24000]\n",
      "loss: 0.257334  [ 3232/24000]\n",
      "loss: 0.239923  [ 6432/24000]\n",
      "loss: 0.284301  [ 9632/24000]\n",
      "loss: 0.366081  [12832/24000]\n",
      "loss: 0.201144  [16032/24000]\n",
      "loss: 0.217144  [19232/24000]\n",
      "loss: 0.373258  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.285548 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.358267  [   32/24000]\n",
      "loss: 0.315753  [ 3232/24000]\n",
      "loss: 0.455692  [ 6432/24000]\n",
      "loss: 0.216016  [ 9632/24000]\n",
      "loss: 0.340577  [12832/24000]\n",
      "loss: 0.386525  [16032/24000]\n",
      "loss: 0.277194  [19232/24000]\n",
      "loss: 0.275102  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.280106 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.303016  [   32/24000]\n",
      "loss: 0.206413  [ 3232/24000]\n",
      "loss: 0.266113  [ 6432/24000]\n",
      "loss: 0.378885  [ 9632/24000]\n",
      "loss: 0.211295  [12832/24000]\n",
      "loss: 0.227752  [16032/24000]\n",
      "loss: 0.319184  [19232/24000]\n",
      "loss: 0.207107  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.280921 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.430967  [   32/24000]\n",
      "loss: 0.371234  [ 3232/24000]\n",
      "loss: 0.201490  [ 6432/24000]\n",
      "loss: 0.347057  [ 9632/24000]\n",
      "loss: 0.447876  [12832/24000]\n",
      "loss: 0.275446  [16032/24000]\n",
      "loss: 0.208572  [19232/24000]\n",
      "loss: 0.350845  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.276497 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.190117  [   32/24000]\n",
      "loss: 0.216818  [ 3232/24000]\n",
      "loss: 0.431232  [ 6432/24000]\n",
      "loss: 0.965181  [ 9632/24000]\n",
      "loss: 0.118291  [12832/24000]\n",
      "loss: 0.328498  [16032/24000]\n",
      "loss: 0.271155  [19232/24000]\n",
      "loss: 0.324749  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.275272 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.330040  [   32/24000]\n",
      "loss: 0.289941  [ 3232/24000]\n",
      "loss: 0.739635  [ 6432/24000]\n",
      "loss: 0.350448  [ 9632/24000]\n",
      "loss: 0.483841  [12832/24000]\n",
      "loss: 0.248387  [16032/24000]\n",
      "loss: 0.165657  [19232/24000]\n",
      "loss: 0.235336  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.277165 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.397204  [   32/24000]\n",
      "loss: 0.235324  [ 3232/24000]\n",
      "loss: 0.269353  [ 6432/24000]\n",
      "loss: 0.327877  [ 9632/24000]\n",
      "loss: 0.298950  [12832/24000]\n",
      "loss: 0.236684  [16032/24000]\n",
      "loss: 0.158024  [19232/24000]\n",
      "loss: 0.260140  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.272243 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.342266  [   32/24000]\n",
      "loss: 0.422686  [ 3232/24000]\n",
      "loss: 0.379562  [ 6432/24000]\n",
      "loss: 0.254612  [ 9632/24000]\n",
      "loss: 0.254825  [12832/24000]\n",
      "loss: 0.303273  [16032/24000]\n",
      "loss: 0.165314  [19232/24000]\n",
      "loss: 0.220671  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.273926 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.253442  [   32/24000]\n",
      "loss: 0.160582  [ 3232/24000]\n",
      "loss: 0.499174  [ 6432/24000]\n",
      "loss: 0.388551  [ 9632/24000]\n",
      "loss: 0.177839  [12832/24000]\n",
      "loss: 0.352958  [16032/24000]\n",
      "loss: 0.193528  [19232/24000]\n",
      "loss: 0.368729  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.274941 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.405432  [   32/24000]\n",
      "loss: 0.194847  [ 3232/24000]\n",
      "loss: 0.288842  [ 6432/24000]\n",
      "loss: 0.338471  [ 9632/24000]\n",
      "loss: 0.248217  [12832/24000]\n",
      "loss: 0.342021  [16032/24000]\n",
      "loss: 0.252213  [19232/24000]\n",
      "loss: 0.324430  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.270884 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.279273  [   32/24000]\n",
      "loss: 0.240556  [ 3232/24000]\n",
      "loss: 0.201919  [ 6432/24000]\n",
      "loss: 0.328682  [ 9632/24000]\n",
      "loss: 0.237055  [12832/24000]\n",
      "loss: 0.503201  [16032/24000]\n",
      "loss: 0.254396  [19232/24000]\n",
      "loss: 0.441004  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.270995 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.211690  [   32/24000]\n",
      "loss: 0.217025  [ 3232/24000]\n",
      "loss: 0.208688  [ 6432/24000]\n",
      "loss: 0.148272  [ 9632/24000]\n",
      "loss: 0.219482  [12832/24000]\n",
      "loss: 0.257632  [16032/24000]\n",
      "loss: 0.211595  [19232/24000]\n",
      "loss: 0.456555  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.269186 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.207368  [   32/24000]\n",
      "loss: 0.384877  [ 3232/24000]\n",
      "loss: 0.138142  [ 6432/24000]\n",
      "loss: 0.239146  [ 9632/24000]\n",
      "loss: 0.183992  [12832/24000]\n",
      "loss: 0.138278  [16032/24000]\n",
      "loss: 0.279811  [19232/24000]\n",
      "loss: 0.353433  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.266874 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.174273  [   32/24000]\n",
      "loss: 0.246046  [ 3232/24000]\n",
      "loss: 0.281940  [ 6432/24000]\n",
      "loss: 0.351791  [ 9632/24000]\n",
      "loss: 0.363815  [12832/24000]\n",
      "loss: 0.197875  [16032/24000]\n",
      "loss: 0.267287  [19232/24000]\n",
      "loss: 0.242416  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.265181 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.335100  [   32/24000]\n",
      "loss: 0.331534  [ 3232/24000]\n",
      "loss: 0.220226  [ 6432/24000]\n",
      "loss: 0.299516  [ 9632/24000]\n",
      "loss: 0.253427  [12832/24000]\n",
      "loss: 0.165804  [16032/24000]\n",
      "loss: 0.202741  [19232/24000]\n",
      "loss: 0.317381  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.264552 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.199071  [   32/24000]\n",
      "loss: 0.187300  [ 3232/24000]\n",
      "loss: 0.505231  [ 6432/24000]\n",
      "loss: 0.249785  [ 9632/24000]\n",
      "loss: 0.204954  [12832/24000]\n",
      "loss: 0.298836  [16032/24000]\n",
      "loss: 0.395614  [19232/24000]\n",
      "loss: 0.156729  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.263115 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.233186  [   32/24000]\n",
      "loss: 0.345409  [ 3232/24000]\n",
      "loss: 0.369122  [ 6432/24000]\n",
      "loss: 0.247901  [ 9632/24000]\n",
      "loss: 0.365762  [12832/24000]\n",
      "loss: 0.251740  [16032/24000]\n",
      "loss: 0.322713  [19232/24000]\n",
      "loss: 0.399977  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.263500 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.323173  [   32/24000]\n",
      "loss: 0.212152  [ 3232/24000]\n",
      "loss: 0.235665  [ 6432/24000]\n",
      "loss: 0.324600  [ 9632/24000]\n",
      "loss: 0.300751  [12832/24000]\n",
      "loss: 0.294649  [16032/24000]\n",
      "loss: 0.308243  [19232/24000]\n",
      "loss: 0.254928  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.275556 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.391497  [   32/24000]\n",
      "loss: 0.162217  [ 3232/24000]\n",
      "loss: 0.270450  [ 6432/24000]\n",
      "loss: 0.371884  [ 9632/24000]\n",
      "loss: 0.256438  [12832/24000]\n",
      "loss: 0.189350  [16032/24000]\n",
      "loss: 0.195049  [19232/24000]\n",
      "loss: 0.396587  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.258739 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.284225  [   32/24000]\n",
      "loss: 0.329193  [ 3232/24000]\n",
      "loss: 0.131422  [ 6432/24000]\n",
      "loss: 0.128820  [ 9632/24000]\n",
      "loss: 0.234366  [12832/24000]\n",
      "loss: 0.280773  [16032/24000]\n",
      "loss: 0.249184  [19232/24000]\n",
      "loss: 0.169459  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.260533 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.182693  [   32/24000]\n",
      "loss: 0.374789  [ 3232/24000]\n",
      "loss: 0.180336  [ 6432/24000]\n",
      "loss: 0.238056  [ 9632/24000]\n",
      "loss: 0.251977  [12832/24000]\n",
      "loss: 0.164871  [16032/24000]\n",
      "loss: 0.180818  [19232/24000]\n",
      "loss: 0.170352  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.265447 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.309201  [   32/24000]\n",
      "loss: 0.318878  [ 3232/24000]\n",
      "loss: 0.181607  [ 6432/24000]\n",
      "loss: 0.352079  [ 9632/24000]\n",
      "loss: 0.257932  [12832/24000]\n",
      "loss: 0.135305  [16032/24000]\n",
      "loss: 0.327831  [19232/24000]\n",
      "loss: 0.308018  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.256129 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.224176  [   32/24000]\n",
      "loss: 0.359124  [ 3232/24000]\n",
      "loss: 0.367572  [ 6432/24000]\n",
      "loss: 0.241810  [ 9632/24000]\n",
      "loss: 0.380518  [12832/24000]\n",
      "loss: 0.248092  [16032/24000]\n",
      "loss: 0.361830  [19232/24000]\n",
      "loss: 0.099721  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.254568 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.348018  [   32/24000]\n",
      "loss: 0.329719  [ 3232/24000]\n",
      "loss: 0.185042  [ 6432/24000]\n",
      "loss: 0.260044  [ 9632/24000]\n",
      "loss: 0.162391  [12832/24000]\n",
      "loss: 0.188002  [16032/24000]\n",
      "loss: 0.282280  [19232/24000]\n",
      "loss: 0.185479  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.253253 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.259864  [   32/24000]\n",
      "loss: 0.202167  [ 3232/24000]\n",
      "loss: 0.154643  [ 6432/24000]\n",
      "loss: 0.262679  [ 9632/24000]\n",
      "loss: 0.209463  [12832/24000]\n",
      "loss: 0.325052  [16032/24000]\n",
      "loss: 0.370287  [19232/24000]\n",
      "loss: 0.235926  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.252194 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.347515  [   32/24000]\n",
      "loss: 0.155309  [ 3232/24000]\n",
      "loss: 0.244934  [ 6432/24000]\n",
      "loss: 0.308207  [ 9632/24000]\n",
      "loss: 0.235199  [12832/24000]\n",
      "loss: 0.250816  [16032/24000]\n",
      "loss: 0.196098  [19232/24000]\n",
      "loss: 0.180262  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.253094 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.244079  [   32/24000]\n",
      "loss: 0.333202  [ 3232/24000]\n",
      "loss: 0.403576  [ 6432/24000]\n",
      "loss: 0.173725  [ 9632/24000]\n",
      "loss: 0.366515  [12832/24000]\n",
      "loss: 0.318218  [16032/24000]\n",
      "loss: 0.224038  [19232/24000]\n",
      "loss: 0.194405  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.253414 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.156545  [   32/24000]\n",
      "loss: 0.317324  [ 3232/24000]\n",
      "loss: 0.202318  [ 6432/24000]\n",
      "loss: 0.284325  [ 9632/24000]\n",
      "loss: 0.142104  [12832/24000]\n",
      "loss: 0.441551  [16032/24000]\n",
      "loss: 0.162762  [19232/24000]\n",
      "loss: 0.270249  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.251599 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.207656  [   32/24000]\n",
      "loss: 0.268146  [ 3232/24000]\n",
      "loss: 0.209767  [ 6432/24000]\n",
      "loss: 0.213488  [ 9632/24000]\n",
      "loss: 0.202812  [12832/24000]\n",
      "loss: 0.206203  [16032/24000]\n",
      "loss: 0.187185  [19232/24000]\n",
      "loss: 0.293919  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.249530 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.089405  [   32/24000]\n",
      "loss: 0.323820  [ 3232/24000]\n",
      "loss: 0.183327  [ 6432/24000]\n",
      "loss: 0.296221  [ 9632/24000]\n",
      "loss: 0.341747  [12832/24000]\n",
      "loss: 0.302176  [16032/24000]\n",
      "loss: 0.221170  [19232/24000]\n",
      "loss: 0.307717  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.249332 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.340706  [   32/24000]\n",
      "loss: 0.123234  [ 3232/24000]\n",
      "loss: 0.205381  [ 6432/24000]\n",
      "loss: 0.240823  [ 9632/24000]\n",
      "loss: 0.381722  [12832/24000]\n",
      "loss: 0.170530  [16032/24000]\n",
      "loss: 0.354504  [19232/24000]\n",
      "loss: 0.263126  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.247062 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.191815  [   32/24000]\n",
      "loss: 0.108816  [ 3232/24000]\n",
      "loss: 0.326953  [ 6432/24000]\n",
      "loss: 0.366377  [ 9632/24000]\n",
      "loss: 0.211784  [12832/24000]\n",
      "loss: 0.109222  [16032/24000]\n",
      "loss: 0.139669  [19232/24000]\n",
      "loss: 0.427327  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.247447 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.217066  [   32/24000]\n",
      "loss: 0.252862  [ 3232/24000]\n",
      "loss: 0.452176  [ 6432/24000]\n",
      "loss: 0.313803  [ 9632/24000]\n",
      "loss: 0.211352  [12832/24000]\n",
      "loss: 0.245814  [16032/24000]\n",
      "loss: 0.289282  [19232/24000]\n",
      "loss: 0.559924  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.245122 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.198573  [   32/24000]\n",
      "loss: 0.303899  [ 3232/24000]\n",
      "loss: 0.275574  [ 6432/24000]\n",
      "loss: 0.195557  [ 9632/24000]\n",
      "loss: 0.300983  [12832/24000]\n",
      "loss: 0.261900  [16032/24000]\n",
      "loss: 0.153341  [19232/24000]\n",
      "loss: 0.297882  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.246118 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.388457  [   32/24000]\n",
      "loss: 0.262636  [ 3232/24000]\n",
      "loss: 0.321272  [ 6432/24000]\n",
      "loss: 0.207534  [ 9632/24000]\n",
      "loss: 0.227729  [12832/24000]\n",
      "loss: 0.307022  [16032/24000]\n",
      "loss: 0.411577  [19232/24000]\n",
      "loss: 0.364775  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.252685 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.440165  [   32/24000]\n",
      "loss: 0.236549  [ 3232/24000]\n",
      "loss: 0.226216  [ 6432/24000]\n",
      "loss: 0.160572  [ 9632/24000]\n",
      "loss: 0.102004  [12832/24000]\n",
      "loss: 0.224202  [16032/24000]\n",
      "loss: 0.245778  [19232/24000]\n",
      "loss: 0.082438  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.244243 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.246912  [   32/24000]\n",
      "loss: 0.322590  [ 3232/24000]\n",
      "loss: 0.339368  [ 6432/24000]\n",
      "loss: 0.430904  [ 9632/24000]\n",
      "loss: 0.291340  [12832/24000]\n",
      "loss: 0.148819  [16032/24000]\n",
      "loss: 0.159950  [19232/24000]\n",
      "loss: 0.161712  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.243995 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.345517  [   32/24000]\n",
      "loss: 0.242471  [ 3232/24000]\n",
      "loss: 0.248007  [ 6432/24000]\n",
      "loss: 0.378746  [ 9632/24000]\n",
      "loss: 0.356456  [12832/24000]\n",
      "loss: 0.286664  [16032/24000]\n",
      "loss: 0.215046  [19232/24000]\n",
      "loss: 0.176857  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.242112 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.172401  [   32/24000]\n",
      "loss: 0.147473  [ 3232/24000]\n",
      "loss: 0.475100  [ 6432/24000]\n",
      "loss: 0.156056  [ 9632/24000]\n",
      "loss: 0.253888  [12832/24000]\n",
      "loss: 0.198127  [16032/24000]\n",
      "loss: 0.261134  [19232/24000]\n",
      "loss: 0.428491  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.241344 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.239404  [   32/24000]\n",
      "loss: 0.230088  [ 3232/24000]\n",
      "loss: 0.303397  [ 6432/24000]\n",
      "loss: 0.204251  [ 9632/24000]\n",
      "loss: 0.132123  [12832/24000]\n",
      "loss: 0.165807  [16032/24000]\n",
      "loss: 0.287097  [19232/24000]\n",
      "loss: 0.197551  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.240001 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.248235  [   32/24000]\n",
      "loss: 0.163664  [ 3232/24000]\n",
      "loss: 0.153978  [ 6432/24000]\n",
      "loss: 0.133741  [ 9632/24000]\n",
      "loss: 0.340438  [12832/24000]\n",
      "loss: 0.219072  [16032/24000]\n",
      "loss: 0.330820  [19232/24000]\n",
      "loss: 0.415104  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.240441 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.289222  [   32/24000]\n",
      "loss: 0.136506  [ 3232/24000]\n",
      "loss: 0.119437  [ 6432/24000]\n",
      "loss: 0.399751  [ 9632/24000]\n",
      "loss: 0.264147  [12832/24000]\n",
      "loss: 0.152493  [16032/24000]\n",
      "loss: 0.221548  [19232/24000]\n",
      "loss: 0.298785  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.237834 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.371372  [   32/24000]\n",
      "loss: 0.318034  [ 3232/24000]\n",
      "loss: 0.346999  [ 6432/24000]\n",
      "loss: 0.144536  [ 9632/24000]\n",
      "loss: 0.225284  [12832/24000]\n",
      "loss: 0.317171  [16032/24000]\n",
      "loss: 0.256026  [19232/24000]\n",
      "loss: 0.439993  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.235948 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.255679  [   32/24000]\n",
      "loss: 0.210299  [ 3232/24000]\n",
      "loss: 0.307587  [ 6432/24000]\n",
      "loss: 0.293932  [ 9632/24000]\n",
      "loss: 0.256487  [12832/24000]\n",
      "loss: 0.379339  [16032/24000]\n",
      "loss: 0.377072  [19232/24000]\n",
      "loss: 0.294608  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.235252 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.261045  [   32/24000]\n",
      "loss: 0.307176  [ 3232/24000]\n",
      "loss: 0.315021  [ 6432/24000]\n",
      "loss: 0.219425  [ 9632/24000]\n",
      "loss: 0.226365  [12832/24000]\n",
      "loss: 0.271746  [16032/24000]\n",
      "loss: 0.276145  [19232/24000]\n",
      "loss: 0.210402  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.235167 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.269669  [   32/24000]\n",
      "loss: 0.386011  [ 3232/24000]\n",
      "loss: 0.161328  [ 6432/24000]\n",
      "loss: 0.245409  [ 9632/24000]\n",
      "loss: 0.141121  [12832/24000]\n",
      "loss: 0.218331  [16032/24000]\n",
      "loss: 0.311282  [19232/24000]\n",
      "loss: 0.164821  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.234858 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.212980  [   32/24000]\n",
      "loss: 0.310427  [ 3232/24000]\n",
      "loss: 0.328424  [ 6432/24000]\n",
      "loss: 0.221007  [ 9632/24000]\n",
      "loss: 0.186326  [12832/24000]\n",
      "loss: 0.155662  [16032/24000]\n",
      "loss: 0.146131  [19232/24000]\n",
      "loss: 0.395947  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.232812 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.098288  [   32/24000]\n",
      "loss: 0.303450  [ 3232/24000]\n",
      "loss: 0.531739  [ 6432/24000]\n",
      "loss: 0.334967  [ 9632/24000]\n",
      "loss: 0.193208  [12832/24000]\n",
      "loss: 0.301581  [16032/24000]\n",
      "loss: 0.187300  [19232/24000]\n",
      "loss: 0.255727  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.231786 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.200543  [   32/24000]\n",
      "loss: 0.110004  [ 3232/24000]\n",
      "loss: 0.286232  [ 6432/24000]\n",
      "loss: 0.281667  [ 9632/24000]\n",
      "loss: 0.251601  [12832/24000]\n",
      "loss: 0.238876  [16032/24000]\n",
      "loss: 0.217673  [19232/24000]\n",
      "loss: 0.238724  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.233530 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.104306  [   32/24000]\n",
      "loss: 0.210607  [ 3232/24000]\n",
      "loss: 0.258444  [ 6432/24000]\n",
      "loss: 0.266492  [ 9632/24000]\n",
      "loss: 0.238573  [12832/24000]\n",
      "loss: 0.258349  [16032/24000]\n",
      "loss: 0.224679  [19232/24000]\n",
      "loss: 0.294037  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.233769 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.158465  [   32/24000]\n",
      "loss: 0.464491  [ 3232/24000]\n",
      "loss: 0.215850  [ 6432/24000]\n",
      "loss: 0.266065  [ 9632/24000]\n",
      "loss: 0.241004  [12832/24000]\n",
      "loss: 0.263578  [16032/24000]\n",
      "loss: 0.135519  [19232/24000]\n",
      "loss: 0.196478  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.230116 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.218504  [   32/24000]\n",
      "loss: 0.214311  [ 3232/24000]\n",
      "loss: 0.272946  [ 6432/24000]\n",
      "loss: 0.146550  [ 9632/24000]\n",
      "loss: 0.267531  [12832/24000]\n",
      "loss: 0.300113  [16032/24000]\n",
      "loss: 0.193607  [19232/24000]\n",
      "loss: 0.361841  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.229624 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.252653  [   32/24000]\n",
      "loss: 0.191871  [ 3232/24000]\n",
      "loss: 0.329467  [ 6432/24000]\n",
      "loss: 0.187315  [ 9632/24000]\n",
      "loss: 0.124397  [12832/24000]\n",
      "loss: 0.336610  [16032/24000]\n",
      "loss: 0.340894  [19232/24000]\n",
      "loss: 0.191718  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.243693 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.332584  [   32/24000]\n",
      "loss: 0.258204  [ 3232/24000]\n",
      "loss: 0.246652  [ 6432/24000]\n",
      "loss: 0.182937  [ 9632/24000]\n",
      "loss: 0.237561  [12832/24000]\n",
      "loss: 0.177401  [16032/24000]\n",
      "loss: 0.145649  [19232/24000]\n",
      "loss: 0.154244  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.233964 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.180050  [   32/24000]\n",
      "loss: 0.211827  [ 3232/24000]\n",
      "loss: 0.226798  [ 6432/24000]\n",
      "loss: 0.267661  [ 9632/24000]\n",
      "loss: 0.194186  [12832/24000]\n",
      "loss: 0.289058  [16032/24000]\n",
      "loss: 0.137620  [19232/24000]\n",
      "loss: 0.212668  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.227458 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.159804  [   32/24000]\n",
      "loss: 0.127184  [ 3232/24000]\n",
      "loss: 0.385010  [ 6432/24000]\n",
      "loss: 0.220283  [ 9632/24000]\n",
      "loss: 0.233646  [12832/24000]\n",
      "loss: 0.311271  [16032/24000]\n",
      "loss: 0.306357  [19232/24000]\n",
      "loss: 0.304930  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.227191 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.231677  [   32/24000]\n",
      "loss: 0.186254  [ 3232/24000]\n",
      "loss: 0.222900  [ 6432/24000]\n",
      "loss: 0.073506  [ 9632/24000]\n",
      "loss: 0.196741  [12832/24000]\n",
      "loss: 0.162286  [16032/24000]\n",
      "loss: 0.270470  [19232/24000]\n",
      "loss: 0.171711  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.226433 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.169198  [   32/24000]\n",
      "loss: 0.143278  [ 3232/24000]\n",
      "loss: 0.231373  [ 6432/24000]\n",
      "loss: 0.249907  [ 9632/24000]\n",
      "loss: 0.168987  [12832/24000]\n",
      "loss: 0.093741  [16032/24000]\n",
      "loss: 0.149302  [19232/24000]\n",
      "loss: 0.176344  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.228072 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.243793  [   32/24000]\n",
      "loss: 0.162421  [ 3232/24000]\n",
      "loss: 0.238646  [ 6432/24000]\n",
      "loss: 0.129760  [ 9632/24000]\n",
      "loss: 0.183069  [12832/24000]\n",
      "loss: 0.129421  [16032/24000]\n",
      "loss: 0.227172  [19232/24000]\n",
      "loss: 0.193732  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.223399 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.209333  [   32/24000]\n",
      "loss: 0.282490  [ 3232/24000]\n",
      "loss: 0.221802  [ 6432/24000]\n",
      "loss: 0.237388  [ 9632/24000]\n",
      "loss: 0.112312  [12832/24000]\n",
      "loss: 0.235562  [16032/24000]\n",
      "loss: 0.176516  [19232/24000]\n",
      "loss: 0.287479  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.229477 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.171725  [   32/24000]\n",
      "loss: 0.163052  [ 3232/24000]\n",
      "loss: 0.256805  [ 6432/24000]\n",
      "loss: 0.206362  [ 9632/24000]\n",
      "loss: 0.313554  [12832/24000]\n",
      "loss: 0.322228  [16032/24000]\n",
      "loss: 0.172514  [19232/24000]\n",
      "loss: 0.203413  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.222517 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.219341  [   32/24000]\n",
      "loss: 0.143562  [ 3232/24000]\n",
      "loss: 0.095346  [ 6432/24000]\n",
      "loss: 0.301537  [ 9632/24000]\n",
      "loss: 0.313147  [12832/24000]\n",
      "loss: 0.271286  [16032/24000]\n",
      "loss: 0.182091  [19232/24000]\n",
      "loss: 0.207876  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.222457 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.183352  [   32/24000]\n",
      "loss: 0.249236  [ 3232/24000]\n",
      "loss: 0.179881  [ 6432/24000]\n",
      "loss: 0.221552  [ 9632/24000]\n",
      "loss: 0.357402  [12832/24000]\n",
      "loss: 0.171219  [16032/24000]\n",
      "loss: 0.221372  [19232/24000]\n",
      "loss: 0.197404  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.234288 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.301854  [   32/24000]\n",
      "loss: 0.172647  [ 3232/24000]\n",
      "loss: 0.306064  [ 6432/24000]\n",
      "loss: 0.224240  [ 9632/24000]\n",
      "loss: 0.267258  [12832/24000]\n",
      "loss: 0.137175  [16032/24000]\n",
      "loss: 0.194452  [19232/24000]\n",
      "loss: 0.253696  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.221599 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.436659  [   32/24000]\n",
      "loss: 0.162986  [ 3232/24000]\n",
      "loss: 0.124667  [ 6432/24000]\n",
      "loss: 0.178878  [ 9632/24000]\n",
      "loss: 0.215857  [12832/24000]\n",
      "loss: 0.199247  [16032/24000]\n",
      "loss: 0.249466  [19232/24000]\n",
      "loss: 0.289216  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.221920 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.228273  [   32/24000]\n",
      "loss: 0.274581  [ 3232/24000]\n",
      "loss: 0.186619  [ 6432/24000]\n",
      "loss: 0.206347  [ 9632/24000]\n",
      "loss: 0.358309  [12832/24000]\n",
      "loss: 0.157795  [16032/24000]\n",
      "loss: 0.128553  [19232/24000]\n",
      "loss: 0.257333  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.222073 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.251562  [   32/24000]\n",
      "loss: 0.240447  [ 3232/24000]\n",
      "loss: 0.178297  [ 6432/24000]\n",
      "loss: 0.104505  [ 9632/24000]\n",
      "loss: 0.253734  [12832/24000]\n",
      "loss: 0.130223  [16032/24000]\n",
      "loss: 0.224517  [19232/24000]\n",
      "loss: 0.242204  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.220793 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.145208  [   32/24000]\n",
      "loss: 0.174287  [ 3232/24000]\n",
      "loss: 0.293353  [ 6432/24000]\n",
      "loss: 0.429446  [ 9632/24000]\n",
      "loss: 0.383829  [12832/24000]\n",
      "loss: 0.216703  [16032/24000]\n",
      "loss: 0.157464  [19232/24000]\n",
      "loss: 0.228790  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.218966 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.257017  [   32/24000]\n",
      "loss: 0.283287  [ 3232/24000]\n",
      "loss: 0.160517  [ 6432/24000]\n",
      "loss: 0.124463  [ 9632/24000]\n",
      "loss: 0.246154  [12832/24000]\n",
      "loss: 0.233097  [16032/24000]\n",
      "loss: 0.207319  [19232/24000]\n",
      "loss: 0.341271  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.219718 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.314651  [   32/24000]\n",
      "loss: 0.113808  [ 3232/24000]\n",
      "loss: 0.375165  [ 6432/24000]\n",
      "loss: 0.302641  [ 9632/24000]\n",
      "loss: 0.105673  [12832/24000]\n",
      "loss: 0.085196  [16032/24000]\n",
      "loss: 0.202896  [19232/24000]\n",
      "loss: 0.317617  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.217621 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.155672  [   32/24000]\n",
      "loss: 0.213194  [ 3232/24000]\n",
      "loss: 0.437987  [ 6432/24000]\n",
      "loss: 0.236579  [ 9632/24000]\n",
      "loss: 0.180363  [12832/24000]\n",
      "loss: 0.460791  [16032/24000]\n",
      "loss: 0.118790  [19232/24000]\n",
      "loss: 0.219945  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.216900 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.232039  [   32/24000]\n",
      "loss: 0.146642  [ 3232/24000]\n",
      "loss: 0.279312  [ 6432/24000]\n",
      "loss: 0.246772  [ 9632/24000]\n",
      "loss: 0.306174  [12832/24000]\n",
      "loss: 0.345246  [16032/24000]\n",
      "loss: 0.239778  [19232/24000]\n",
      "loss: 0.268755  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.227949 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.196898  [   32/24000]\n",
      "loss: 0.178057  [ 3232/24000]\n",
      "loss: 0.333689  [ 6432/24000]\n",
      "loss: 0.171615  [ 9632/24000]\n",
      "loss: 0.223722  [12832/24000]\n",
      "loss: 0.131563  [16032/24000]\n",
      "loss: 0.258157  [19232/24000]\n",
      "loss: 0.426156  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.215064 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.238933  [   32/24000]\n",
      "loss: 0.148638  [ 3232/24000]\n",
      "loss: 0.144025  [ 6432/24000]\n",
      "loss: 0.198252  [ 9632/24000]\n",
      "loss: 0.259213  [12832/24000]\n",
      "loss: 0.243861  [16032/24000]\n",
      "loss: 0.237906  [19232/24000]\n",
      "loss: 0.240728  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.218677 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.217709  [   32/24000]\n",
      "loss: 0.406342  [ 3232/24000]\n",
      "loss: 0.289140  [ 6432/24000]\n",
      "loss: 0.216999  [ 9632/24000]\n",
      "loss: 0.173984  [12832/24000]\n",
      "loss: 0.368183  [16032/24000]\n",
      "loss: 0.327943  [19232/24000]\n",
      "loss: 0.249298  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.233031 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.226094  [   32/24000]\n",
      "loss: 0.285149  [ 3232/24000]\n",
      "loss: 0.160308  [ 6432/24000]\n",
      "loss: 0.127007  [ 9632/24000]\n",
      "loss: 0.171457  [12832/24000]\n",
      "loss: 0.326249  [16032/24000]\n",
      "loss: 0.183026  [19232/24000]\n",
      "loss: 0.120957  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.216483 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.222718  [   32/24000]\n",
      "loss: 0.311696  [ 3232/24000]\n",
      "loss: 0.119191  [ 6432/24000]\n",
      "loss: 0.238315  [ 9632/24000]\n",
      "loss: 0.392850  [12832/24000]\n",
      "loss: 0.088690  [16032/24000]\n",
      "loss: 0.167404  [19232/24000]\n",
      "loss: 0.137618  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.217680 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.156823  [   32/24000]\n",
      "loss: 0.102783  [ 3232/24000]\n",
      "loss: 0.255158  [ 6432/24000]\n",
      "loss: 0.204461  [ 9632/24000]\n",
      "loss: 0.259556  [12832/24000]\n",
      "loss: 0.303258  [16032/24000]\n",
      "loss: 0.221363  [19232/24000]\n",
      "loss: 0.067576  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.216513 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.313979  [   32/24000]\n",
      "loss: 0.259368  [ 3232/24000]\n",
      "loss: 0.124298  [ 6432/24000]\n",
      "loss: 0.230820  [ 9632/24000]\n",
      "loss: 0.124594  [12832/24000]\n",
      "loss: 0.168450  [16032/24000]\n",
      "loss: 0.301576  [19232/24000]\n",
      "loss: 0.233858  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.216962 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.348407  [   32/24000]\n",
      "loss: 0.131609  [ 3232/24000]\n",
      "loss: 0.240228  [ 6432/24000]\n",
      "loss: 0.316608  [ 9632/24000]\n",
      "loss: 0.163390  [12832/24000]\n",
      "loss: 0.132859  [16032/24000]\n",
      "loss: 0.163017  [19232/24000]\n",
      "loss: 0.415822  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.211338 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.283401  [   32/24000]\n",
      "loss: 0.355718  [ 3232/24000]\n",
      "loss: 0.329977  [ 6432/24000]\n",
      "loss: 0.296878  [ 9632/24000]\n",
      "loss: 0.114647  [12832/24000]\n",
      "loss: 0.129146  [16032/24000]\n",
      "loss: 0.154156  [19232/24000]\n",
      "loss: 0.355851  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.215010 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.322363  [   32/24000]\n",
      "loss: 0.216437  [ 3232/24000]\n",
      "loss: 0.201548  [ 6432/24000]\n",
      "loss: 0.185738  [ 9632/24000]\n",
      "loss: 0.191354  [12832/24000]\n",
      "loss: 0.273135  [16032/24000]\n",
      "loss: 0.218877  [19232/24000]\n",
      "loss: 0.255835  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.210705 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.205906  [   32/24000]\n",
      "loss: 0.179167  [ 3232/24000]\n",
      "loss: 0.392244  [ 6432/24000]\n",
      "loss: 0.240585  [ 9632/24000]\n",
      "loss: 0.143144  [12832/24000]\n",
      "loss: 0.186004  [16032/24000]\n",
      "loss: 0.243099  [19232/24000]\n",
      "loss: 0.162692  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.214070 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.161563  [   32/24000]\n",
      "loss: 0.143674  [ 3232/24000]\n",
      "loss: 0.261199  [ 6432/24000]\n",
      "loss: 0.222917  [ 9632/24000]\n",
      "loss: 0.191815  [12832/24000]\n",
      "loss: 0.268166  [16032/24000]\n",
      "loss: 0.082135  [19232/24000]\n",
      "loss: 0.181648  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.210082 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.270615  [   32/24000]\n",
      "loss: 0.084927  [ 3232/24000]\n",
      "loss: 0.251972  [ 6432/24000]\n",
      "loss: 0.304240  [ 9632/24000]\n",
      "loss: 0.222520  [12832/24000]\n",
      "loss: 0.276243  [16032/24000]\n",
      "loss: 0.307825  [19232/24000]\n",
      "loss: 0.273561  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.212216 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.201366  [   32/24000]\n",
      "loss: 0.245764  [ 3232/24000]\n",
      "loss: 0.288281  [ 6432/24000]\n",
      "loss: 0.163088  [ 9632/24000]\n",
      "loss: 0.257306  [12832/24000]\n",
      "loss: 0.273722  [16032/24000]\n",
      "loss: 0.170851  [19232/24000]\n",
      "loss: 0.288861  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.208815 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.223705  [   32/24000]\n",
      "loss: 0.255849  [ 3232/24000]\n",
      "loss: 0.136909  [ 6432/24000]\n",
      "loss: 0.260667  [ 9632/24000]\n",
      "loss: 0.150200  [12832/24000]\n",
      "loss: 0.295561  [16032/24000]\n",
      "loss: 0.148112  [19232/24000]\n",
      "loss: 0.299836  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.214180 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.414965  [   32/24000]\n",
      "loss: 0.091988  [ 3232/24000]\n",
      "loss: 0.270511  [ 6432/24000]\n",
      "loss: 0.135682  [ 9632/24000]\n",
      "loss: 0.272404  [12832/24000]\n",
      "loss: 0.151827  [16032/24000]\n",
      "loss: 0.231936  [19232/24000]\n",
      "loss: 0.182544  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.212998 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.149898  [   32/24000]\n",
      "loss: 0.301946  [ 3232/24000]\n",
      "loss: 0.249823  [ 6432/24000]\n",
      "loss: 0.268700  [ 9632/24000]\n",
      "loss: 0.359749  [12832/24000]\n",
      "loss: 0.171814  [16032/24000]\n",
      "loss: 0.208903  [19232/24000]\n",
      "loss: 0.179341  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.207253 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.118246  [   32/24000]\n",
      "loss: 0.220839  [ 3232/24000]\n",
      "loss: 0.200455  [ 6432/24000]\n",
      "loss: 0.183794  [ 9632/24000]\n",
      "loss: 0.115885  [12832/24000]\n",
      "loss: 0.195494  [16032/24000]\n",
      "loss: 0.186099  [19232/24000]\n",
      "loss: 0.137201  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.207055 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.143051  [   32/24000]\n",
      "loss: 0.143764  [ 3232/24000]\n",
      "loss: 0.281212  [ 6432/24000]\n",
      "loss: 0.219871  [ 9632/24000]\n",
      "loss: 0.187098  [12832/24000]\n",
      "loss: 0.243947  [16032/24000]\n",
      "loss: 0.303317  [19232/24000]\n",
      "loss: 0.253722  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.220579 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.161738  [   32/24000]\n",
      "loss: 0.245795  [ 3232/24000]\n",
      "loss: 0.137161  [ 6432/24000]\n",
      "loss: 0.119320  [ 9632/24000]\n",
      "loss: 0.296401  [12832/24000]\n",
      "loss: 0.248414  [16032/24000]\n",
      "loss: 0.102969  [19232/24000]\n",
      "loss: 0.144831  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.206403 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.168216  [   32/24000]\n",
      "loss: 0.345125  [ 3232/24000]\n",
      "loss: 0.166259  [ 6432/24000]\n",
      "loss: 0.261263  [ 9632/24000]\n",
      "loss: 0.226947  [12832/24000]\n",
      "loss: 0.373761  [16032/24000]\n",
      "loss: 0.090263  [19232/24000]\n",
      "loss: 0.120262  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.208216 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.161091  [   32/24000]\n",
      "loss: 0.153077  [ 3232/24000]\n",
      "loss: 0.144093  [ 6432/24000]\n",
      "loss: 0.126187  [ 9632/24000]\n",
      "loss: 0.100388  [12832/24000]\n",
      "loss: 0.179578  [16032/24000]\n",
      "loss: 0.174703  [19232/24000]\n",
      "loss: 0.190258  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.212475 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.173067  [   32/24000]\n",
      "loss: 0.226942  [ 3232/24000]\n",
      "loss: 0.192466  [ 6432/24000]\n",
      "loss: 0.246369  [ 9632/24000]\n",
      "loss: 0.153382  [12832/24000]\n",
      "loss: 0.204549  [16032/24000]\n",
      "loss: 0.244062  [19232/24000]\n",
      "loss: 0.133936  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.212157 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.183559  [   32/24000]\n",
      "loss: 0.340980  [ 3232/24000]\n",
      "loss: 0.190808  [ 6432/24000]\n",
      "loss: 0.110593  [ 9632/24000]\n",
      "loss: 0.113019  [12832/24000]\n",
      "loss: 0.177545  [16032/24000]\n",
      "loss: 0.183072  [19232/24000]\n",
      "loss: 0.130974  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.219488 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.160451  [   32/24000]\n",
      "loss: 0.136131  [ 3232/24000]\n",
      "loss: 0.161697  [ 6432/24000]\n",
      "loss: 0.320432  [ 9632/24000]\n",
      "loss: 0.290611  [12832/24000]\n",
      "loss: 0.396061  [16032/24000]\n",
      "loss: 0.197533  [19232/24000]\n",
      "loss: 0.361824  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.215639 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.113529  [   32/24000]\n",
      "loss: 0.145509  [ 3232/24000]\n",
      "loss: 0.301011  [ 6432/24000]\n",
      "loss: 0.209537  [ 9632/24000]\n",
      "loss: 0.170812  [12832/24000]\n",
      "loss: 0.258026  [16032/24000]\n",
      "loss: 0.181281  [19232/24000]\n",
      "loss: 0.186075  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.203578 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.094564  [   32/24000]\n",
      "loss: 0.341023  [ 3232/24000]\n",
      "loss: 0.472526  [ 6432/24000]\n",
      "loss: 0.159113  [ 9632/24000]\n",
      "loss: 0.240786  [12832/24000]\n",
      "loss: 0.338060  [16032/24000]\n",
      "loss: 0.084615  [19232/24000]\n",
      "loss: 0.146621  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.204697 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.279619  [   32/24000]\n",
      "loss: 0.310893  [ 3232/24000]\n",
      "loss: 0.154417  [ 6432/24000]\n",
      "loss: 0.285106  [ 9632/24000]\n",
      "loss: 0.101765  [12832/24000]\n",
      "loss: 0.228979  [16032/24000]\n",
      "loss: 0.131961  [19232/24000]\n",
      "loss: 0.189203  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.205434 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.265035  [   32/24000]\n",
      "loss: 0.088267  [ 3232/24000]\n",
      "loss: 0.124568  [ 6432/24000]\n",
      "loss: 0.098571  [ 9632/24000]\n",
      "loss: 0.213878  [12832/24000]\n",
      "loss: 0.109072  [16032/24000]\n",
      "loss: 0.219605  [19232/24000]\n",
      "loss: 0.165498  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.202750 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.227003  [   32/24000]\n",
      "loss: 0.078922  [ 3232/24000]\n",
      "loss: 0.208332  [ 6432/24000]\n",
      "loss: 0.248960  [ 9632/24000]\n",
      "loss: 0.116242  [12832/24000]\n",
      "loss: 0.313369  [16032/24000]\n",
      "loss: 0.287577  [19232/24000]\n",
      "loss: 0.296577  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.207127 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.180696  [   32/24000]\n",
      "loss: 0.293847  [ 3232/24000]\n",
      "loss: 0.250799  [ 6432/24000]\n",
      "loss: 0.255806  [ 9632/24000]\n",
      "loss: 0.100955  [12832/24000]\n",
      "loss: 0.129485  [16032/24000]\n",
      "loss: 0.354455  [19232/24000]\n",
      "loss: 0.279151  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.202705 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.117751  [   32/24000]\n",
      "loss: 0.206771  [ 3232/24000]\n",
      "loss: 0.074264  [ 6432/24000]\n",
      "loss: 0.156342  [ 9632/24000]\n",
      "loss: 0.098599  [12832/24000]\n",
      "loss: 0.176321  [16032/24000]\n",
      "loss: 0.223638  [19232/24000]\n",
      "loss: 0.168389  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.205080 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.159419  [   32/24000]\n",
      "loss: 0.107394  [ 3232/24000]\n",
      "loss: 0.160933  [ 6432/24000]\n",
      "loss: 0.215830  [ 9632/24000]\n",
      "loss: 0.195982  [12832/24000]\n",
      "loss: 0.154961  [16032/24000]\n",
      "loss: 0.119928  [19232/24000]\n",
      "loss: 0.123745  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.206746 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.250948  [   32/24000]\n",
      "loss: 0.128296  [ 3232/24000]\n",
      "loss: 0.668205  [ 6432/24000]\n",
      "loss: 0.206005  [ 9632/24000]\n",
      "loss: 0.124517  [12832/24000]\n",
      "loss: 0.292053  [16032/24000]\n",
      "loss: 0.194083  [19232/24000]\n",
      "loss: 0.315900  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.201069 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.362616  [   32/24000]\n",
      "loss: 0.188330  [ 3232/24000]\n",
      "loss: 0.113699  [ 6432/24000]\n",
      "loss: 0.288924  [ 9632/24000]\n",
      "loss: 0.039692  [12832/24000]\n",
      "loss: 0.134118  [16032/24000]\n",
      "loss: 0.286983  [19232/24000]\n",
      "loss: 0.235991  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.202323 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.050833  [   32/24000]\n",
      "loss: 0.086595  [ 3232/24000]\n",
      "loss: 0.104302  [ 6432/24000]\n",
      "loss: 0.319400  [ 9632/24000]\n",
      "loss: 0.205845  [12832/24000]\n",
      "loss: 0.242001  [16032/24000]\n",
      "loss: 0.079602  [19232/24000]\n",
      "loss: 0.261840  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.212498 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.181281  [   32/24000]\n",
      "loss: 0.237166  [ 3232/24000]\n",
      "loss: 0.129301  [ 6432/24000]\n",
      "loss: 0.138769  [ 9632/24000]\n",
      "loss: 0.187000  [12832/24000]\n",
      "loss: 0.211099  [16032/24000]\n",
      "loss: 0.092844  [19232/24000]\n",
      "loss: 0.129875  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.200159 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.492315  [   32/24000]\n",
      "loss: 0.198781  [ 3232/24000]\n",
      "loss: 0.271303  [ 6432/24000]\n",
      "loss: 0.204520  [ 9632/24000]\n",
      "loss: 0.090285  [12832/24000]\n",
      "loss: 0.225854  [16032/24000]\n",
      "loss: 0.215251  [19232/24000]\n",
      "loss: 0.417167  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.203929 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.135382  [   32/24000]\n",
      "loss: 0.322187  [ 3232/24000]\n",
      "loss: 0.142894  [ 6432/24000]\n",
      "loss: 0.293795  [ 9632/24000]\n",
      "loss: 0.179150  [12832/24000]\n",
      "loss: 0.265163  [16032/24000]\n",
      "loss: 0.181632  [19232/24000]\n",
      "loss: 0.159022  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.203594 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.206460  [   32/24000]\n",
      "loss: 0.209127  [ 3232/24000]\n",
      "loss: 0.160658  [ 6432/24000]\n",
      "loss: 0.123076  [ 9632/24000]\n",
      "loss: 0.255931  [12832/24000]\n",
      "loss: 0.117217  [16032/24000]\n",
      "loss: 0.166979  [19232/24000]\n",
      "loss: 0.202175  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.200977 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.132813  [   32/24000]\n",
      "loss: 0.180863  [ 3232/24000]\n",
      "loss: 0.198066  [ 6432/24000]\n",
      "loss: 0.114035  [ 9632/24000]\n",
      "loss: 0.181257  [12832/24000]\n",
      "loss: 0.172383  [16032/24000]\n",
      "loss: 0.240778  [19232/24000]\n",
      "loss: 0.224617  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.198821 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.178516  [   32/24000]\n",
      "loss: 0.262894  [ 3232/24000]\n",
      "loss: 0.177116  [ 6432/24000]\n",
      "loss: 0.129495  [ 9632/24000]\n",
      "loss: 0.150585  [12832/24000]\n",
      "loss: 0.204893  [16032/24000]\n",
      "loss: 0.193135  [19232/24000]\n",
      "loss: 0.272677  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.203633 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.202296  [   32/24000]\n",
      "loss: 0.200760  [ 3232/24000]\n",
      "loss: 0.133029  [ 6432/24000]\n",
      "loss: 0.196783  [ 9632/24000]\n",
      "loss: 0.222857  [12832/24000]\n",
      "loss: 0.232528  [16032/24000]\n",
      "loss: 0.164376  [19232/24000]\n",
      "loss: 0.179137  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.198698 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.185899  [   32/24000]\n",
      "loss: 0.117294  [ 3232/24000]\n",
      "loss: 0.208857  [ 6432/24000]\n",
      "loss: 0.183665  [ 9632/24000]\n",
      "loss: 0.079557  [12832/24000]\n",
      "loss: 0.344796  [16032/24000]\n",
      "loss: 0.176514  [19232/24000]\n",
      "loss: 0.306849  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.198538 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.129689  [   32/24000]\n",
      "loss: 0.141453  [ 3232/24000]\n",
      "loss: 0.211676  [ 6432/24000]\n",
      "loss: 0.285602  [ 9632/24000]\n",
      "loss: 0.278254  [12832/24000]\n",
      "loss: 0.211558  [16032/24000]\n",
      "loss: 0.318972  [19232/24000]\n",
      "loss: 0.173594  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.198953 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.123425  [   32/24000]\n",
      "loss: 0.211927  [ 3232/24000]\n",
      "loss: 0.111175  [ 6432/24000]\n",
      "loss: 0.200079  [ 9632/24000]\n",
      "loss: 0.176746  [12832/24000]\n",
      "loss: 0.237878  [16032/24000]\n",
      "loss: 0.164714  [19232/24000]\n",
      "loss: 0.184184  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.198596 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.257576  [   32/24000]\n",
      "loss: 0.173217  [ 3232/24000]\n",
      "loss: 0.258358  [ 6432/24000]\n",
      "loss: 0.265228  [ 9632/24000]\n",
      "loss: 0.254386  [12832/24000]\n",
      "loss: 0.120776  [16032/24000]\n",
      "loss: 0.139361  [19232/24000]\n",
      "loss: 0.136990  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.197992 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.279872  [   32/24000]\n",
      "loss: 0.079498  [ 3232/24000]\n",
      "loss: 0.420667  [ 6432/24000]\n",
      "loss: 0.298539  [ 9632/24000]\n",
      "loss: 0.138549  [12832/24000]\n",
      "loss: 0.142359  [16032/24000]\n",
      "loss: 0.101080  [19232/24000]\n",
      "loss: 0.209451  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.198972 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.118118  [   32/24000]\n",
      "loss: 0.211987  [ 3232/24000]\n",
      "loss: 0.145922  [ 6432/24000]\n",
      "loss: 0.199565  [ 9632/24000]\n",
      "loss: 0.196366  [12832/24000]\n",
      "loss: 0.079016  [16032/24000]\n",
      "loss: 0.100059  [19232/24000]\n",
      "loss: 0.153860  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.200648 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.220680  [   32/24000]\n",
      "loss: 0.280220  [ 3232/24000]\n",
      "loss: 0.260505  [ 6432/24000]\n",
      "loss: 0.193246  [ 9632/24000]\n",
      "loss: 0.234745  [12832/24000]\n",
      "loss: 0.188923  [16032/24000]\n",
      "loss: 0.105099  [19232/24000]\n",
      "loss: 0.101071  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.196744 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.188452  [   32/24000]\n",
      "loss: 0.287095  [ 3232/24000]\n",
      "loss: 0.312286  [ 6432/24000]\n",
      "loss: 0.139249  [ 9632/24000]\n",
      "loss: 0.342424  [12832/24000]\n",
      "loss: 0.143795  [16032/24000]\n",
      "loss: 0.326077  [19232/24000]\n",
      "loss: 0.324781  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.199696 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.107502  [   32/24000]\n",
      "loss: 0.225071  [ 3232/24000]\n",
      "loss: 0.141766  [ 6432/24000]\n",
      "loss: 0.284906  [ 9632/24000]\n",
      "loss: 0.170971  [12832/24000]\n",
      "loss: 0.264506  [16032/24000]\n",
      "loss: 0.172600  [19232/24000]\n",
      "loss: 0.142319  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.197967 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.174190  [   32/24000]\n",
      "loss: 0.136889  [ 3232/24000]\n",
      "loss: 0.079791  [ 6432/24000]\n",
      "loss: 0.122434  [ 9632/24000]\n",
      "loss: 0.177953  [12832/24000]\n",
      "loss: 0.205538  [16032/24000]\n",
      "loss: 0.175322  [19232/24000]\n",
      "loss: 0.160382  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.202414 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.228673  [   32/24000]\n",
      "loss: 0.121300  [ 3232/24000]\n",
      "loss: 0.166503  [ 6432/24000]\n",
      "loss: 0.298298  [ 9632/24000]\n",
      "loss: 0.156979  [12832/24000]\n",
      "loss: 0.076301  [16032/24000]\n",
      "loss: 0.095950  [19232/24000]\n",
      "loss: 0.169390  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.195058 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") \n",
    "epochs = 150\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "23995    1\n",
       "23996    1\n",
       "23997    0\n",
       "23998    0\n",
       "23999    0\n",
       "Name: label, Length: 24000, dtype: int64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>1356</td>\n",
       "      <td>127</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1930</td>\n",
       "      <td>180</td>\n",
       "      <td>15</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.205479</td>\n",
       "      <td>3.013699</td>\n",
       "      <td>1.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>1526</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>1</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>5.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "      <td>6187</td>\n",
       "      <td>573</td>\n",
       "      <td>36</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.9899</td>\n",
       "      <td>1</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.523246</td>\n",
       "      <td>1.521555</td>\n",
       "      <td>4.564666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>2517</td>\n",
       "      <td>235</td>\n",
       "      <td>19</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.720978</td>\n",
       "      <td>3.054990</td>\n",
       "      <td>3.665988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "      <td>658</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.8308</td>\n",
       "      <td>1</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.727273</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>4.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>2156</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>1</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.706468</td>\n",
       "      <td>1.741294</td>\n",
       "      <td>1.741294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>4</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>1351</td>\n",
       "      <td>134</td>\n",
       "      <td>10</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>1</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.822511</td>\n",
       "      <td>2.164502</td>\n",
       "      <td>2.164502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>5376</td>\n",
       "      <td>502</td>\n",
       "      <td>32</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.4400</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.578947</td>\n",
       "      <td>2.315789</td>\n",
       "      <td>3.684211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0                3                75                11             1356   \n",
       "1                5                55                 7             1930   \n",
       "2                1                66                 8             1526   \n",
       "3                6                86                 8             6187   \n",
       "4                6                75                 8             2517   \n",
       "...            ...               ...               ...              ...   \n",
       "23995            8                80                14              658   \n",
       "23996            5                67                 8             2156   \n",
       "23997            4               145                 1              145   \n",
       "23998            5                48                 7             1351   \n",
       "23999            1                72                 9             5376   \n",
       "\n",
       "       text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0                  127                    8  0.044  0.892  0.064    0.5411   \n",
       "1                  180                   15  0.135  0.856  0.009   -0.9890   \n",
       "2                  150                    8  0.145  0.708  0.147   -0.4767   \n",
       "3                  573                   36  0.114  0.800  0.086   -0.9899   \n",
       "4                  235                   19  0.073  0.840  0.086    0.5282   \n",
       "...                ...                  ...    ...    ...    ...       ...   \n",
       "23995               69                    4  0.084  0.916  0.000   -0.8308   \n",
       "23996              200                   12  0.042  0.876  0.082    0.9274   \n",
       "23997                1                    1  0.000  1.000  0.000    0.0000   \n",
       "23998              134                   10  0.050  0.877  0.073    0.6360   \n",
       "23999              502                   32  0.072  0.853  0.074   -0.4400   \n",
       "\n",
       "       Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0              0    5.555556         0.00         0.0   9.523810  0.000000   \n",
       "1              1    0.000000         0.00         0.0   5.205479  3.013699   \n",
       "2              1   40.000000         0.00         0.0  13.888889  0.694444   \n",
       "3              1   12.500000         0.00        12.5   7.523246  1.521555   \n",
       "4              1    0.000000         0.00         0.0   6.720978  3.054990   \n",
       "...          ...         ...          ...         ...        ...       ...   \n",
       "23995          1   18.750000         6.25         0.0  12.727273  4.545455   \n",
       "23996          1   40.000000         0.00         0.0   8.706468  1.741294   \n",
       "23997          1   33.333333         0.00         0.0  33.333333  0.000000   \n",
       "23998          1   11.111111         0.00         0.0  10.822511  2.164502   \n",
       "23999          1   33.333333         0.00         0.0   9.578947  2.315789   \n",
       "\n",
       "        RB_text  \n",
       "0      1.298701  \n",
       "1      1.917808  \n",
       "2      5.208333  \n",
       "3      4.564666  \n",
       "4      3.665988  \n",
       "...         ...  \n",
       "23995  4.545455  \n",
       "23996  1.741294  \n",
       "23997  0.000000  \n",
       "23998  2.164502  \n",
       "23999  3.684211  \n",
       "\n",
       "[24000 rows x 17 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():  # Disable gradient calculation\n",
    "#     outputs = model(X_test_tensor)  # Get logits\n",
    "#     probabilities = torch.sigmoid(outputs)  # Apply sigmoid for probabilities\n",
    "#     predictions = (probabilities > 0.5).float()  # Convert to 0 or 1\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(X_test_tensor)  # Get logits (size: [batch_size, 2])\n",
    "    probabilities = torch.softmax(outputs, dim=1)  # Apply softmax to get class probabilities\n",
    "    predictions = torch.argmax(probabilities, dim=1)  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 6000\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.cpu().numpy()\n",
    "y_true = y_test_tensor.view(-1).cpu().numpy()\n",
    "print(len(y_pred), len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.63%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_pred == y_true).sum() / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conservatives Will HATE What Donald Trump Just...</td>\n",
       "      <td>Donald Trump isn t exactly a stranger to makin...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump victory may create new tension between U...</td>\n",
       "      <td>Donald Trumpâ€™s U.S. election victory may creat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...</td>\n",
       "      <td>A couple of quick questions come to mind when ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 9, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic Senator Franken to resign: CNN, cit...</td>\n",
       "      <td>U.S. Democratic Senator Al Franken will announ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GANG OF DOMESTIC TERRORISTS Violently Attack L...</td>\n",
       "      <td>***WARNING*** Violence is graphic***This Trump...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jan 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Conservatives Will HATE What Donald Trump Just...   \n",
       "1  Trump victory may create new tension between U...   \n",
       "2  WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...   \n",
       "3  Democratic Senator Franken to resign: CNN, cit...   \n",
       "4  GANG OF DOMESTIC TERRORISTS Violently Attack L...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump isn t exactly a stranger to makin...          News   \n",
       "1  Donald Trumpâ€™s U.S. election victory may creat...  politicsNews   \n",
       "2  A couple of quick questions come to mind when ...      politics   \n",
       "3  U.S. Democratic Senator Al Franken will announ...  politicsNews   \n",
       "4  ***WARNING*** Violence is graphic***This Trump...     left-news   \n",
       "\n",
       "                date  label  \n",
       "0  February 14, 2016      0  \n",
       "1  November 9, 2016       1  \n",
       "2        Nov 9, 2017      0  \n",
       "3  December 7, 2017       1  \n",
       "4       Jan 21, 2017      0  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns = \"Unnamed: 0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conservatives Will HATE What Donald Trump Just...</td>\n",
       "      <td>Donald Trump isn t exactly a stranger to makin...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump victory may create new tension between U...</td>\n",
       "      <td>Donald Trumpâ€™s U.S. election victory may creat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...</td>\n",
       "      <td>A couple of quick questions come to mind when ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 9, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic Senator Franken to resign: CNN, cit...</td>\n",
       "      <td>U.S. Democratic Senator Al Franken will announ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GANG OF DOMESTIC TERRORISTS Violently Attack L...</td>\n",
       "      <td>***WARNING*** Violence is graphic***This Trump...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jan 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Conservatives Will HATE What Donald Trump Just...   \n",
       "1  Trump victory may create new tension between U...   \n",
       "2  WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...   \n",
       "3  Democratic Senator Franken to resign: CNN, cit...   \n",
       "4  GANG OF DOMESTIC TERRORISTS Violently Attack L...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump isn t exactly a stranger to makin...          News   \n",
       "1  Donald Trumpâ€™s U.S. election victory may creat...  politicsNews   \n",
       "2  A couple of quick questions come to mind when ...      politics   \n",
       "3  U.S. Democratic Senator Al Franken will announ...  politicsNews   \n",
       "4  ***WARNING*** Violence is graphic***This Trump...     left-news   \n",
       "\n",
       "                date  label  \n",
       "0  February 14, 2016      0  \n",
       "1  November 9, 2016       1  \n",
       "2        Nov 9, 2017      0  \n",
       "3  December 7, 2017       1  \n",
       "4       Jan 21, 2017      0  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = pd.read_csv(\"test_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tit_len_test = pd.read_csv(\"test_title_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_length_test = pd.read_csv(\"test_text_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_train_outputs_text_test = pd.read_csv(\"vader_test_outputs_text.csv\")\n",
    "vader_train_outputs_title_test = pd.read_csv(\"vader_test_outputs_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Punctuate, dtype: int64"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuated_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>2290</td>\n",
       "      <td>209</td>\n",
       "      <td>15</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>1</td>\n",
       "      <td>6.733167</td>\n",
       "      <td>2.743142</td>\n",
       "      <td>5.486284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>658</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>1</td>\n",
       "      <td>14.516129</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>11</td>\n",
       "      <td>2644</td>\n",
       "      <td>242</td>\n",
       "      <td>17</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.8980</td>\n",
       "      <td>1</td>\n",
       "      <td>10.267857</td>\n",
       "      <td>2.008929</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>5.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>1</td>\n",
       "      <td>10.638298</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>15</td>\n",
       "      <td>344</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.9206</td>\n",
       "      <td>1</td>\n",
       "      <td>9.677419</td>\n",
       "      <td>4.838710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>4.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0            6                84                 8             2290   \n",
       "1            1                84                11              658   \n",
       "2            4               103                11             2644   \n",
       "3            1                57                 7              279   \n",
       "4            3               123                15              344   \n",
       "\n",
       "   text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0              209                   15  0.109  0.724  0.167    0.9849   \n",
       "1               70                    4  0.102  0.771  0.127    0.4215   \n",
       "2              242                   17  0.082  0.872  0.046   -0.8980   \n",
       "3               27                    1  0.050  0.950  0.000   -0.2960   \n",
       "4               26                    1  0.294  0.612  0.093   -0.9206   \n",
       "\n",
       "   Punctuate    JJ_text  VBG_text   RB_text  JJ_density  VBG_density  \\\n",
       "0          1   6.733167  2.743142  5.486284    0.000000     0.000000   \n",
       "1          1  14.516129  2.419355  2.419355   21.428571     0.000000   \n",
       "2          1  10.267857  2.008929  6.250000   11.764706     5.882353   \n",
       "3          1  10.638298  4.255319  0.000000   10.000000    10.000000   \n",
       "4          1   9.677419  4.838710  0.000000   18.181818     4.545455   \n",
       "\n",
       "   RB_density  \n",
       "0    7.142857  \n",
       "1    0.000000  \n",
       "2    5.882353  \n",
       "3    0.000000  \n",
       "4    4.545455  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test_df = pd.concat([test_cat, test_tit_len_test, train_text_length_test, \n",
    "                       vader_train_outputs_text_test, vader_train_outputs_title_test, punctuated_test_df, keyword_test_text, keyword_test_title], axis=1)\n",
    "merged_test_df = merged_test_df.loc[:, ~merged_test_df.columns.duplicated()]\n",
    "merged_test_df.drop(columns=[\"id\", \"Unnamed: 0\"],inplace=True)\n",
    "merged_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = merged_test_df.values\n",
    "y_test = test_df['label']\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(X_test_tensor)  # Get logits (size: [batch_size, 2])\n",
    "    probabilities = torch.softmax(outputs, dim=1)  # Apply softmax to get class probabilities\n",
    "    predictions = torch.argmax(probabilities, dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8267 8267\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.cpu().numpy()\n",
    "y_true = y_test_tensor.view(-1).cpu().numpy()\n",
    "print(len(y_pred), len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.89%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_pred == y_true).sum() / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      4284\n",
      "           1       0.85      0.81      0.83      3983\n",
      "\n",
      "    accuracy                           0.84      8267\n",
      "   macro avg       0.84      0.84      0.84      8267\n",
      "weighted avg       0.84      0.84      0.84      8267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([8267, 2])\n",
      "y_test_tensor shape: torch.Size([8267, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions shape:\", predictions.shape)       # Before flattening\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)   # Before flattening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16534,) (8267,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16534,) (8267,) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
