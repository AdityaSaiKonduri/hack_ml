{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.tsv\",sep='\\t')\n",
    "test_df = pd.read_csv(\"test.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = pd.read_csv(\"train_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tit_len = pd.read_csv(\"train_title_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_length = pd.read_csv(\"train_text_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_train_outputs_text = pd.read_csv(\"vader_train_outputs_text.csv\")\n",
    "vader_train_outputs_title = pd.read_csv(\"vader_train_outputs_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuated_train_df = pd.read_csv('punctuate_train.csv')\n",
    "punctuated_test_df = pd.read_csv('punctuate_test.csv')\n",
    "punctuated_test_df = punctuated_test_df['Punctuate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_train_title = pd.read_csv('title_keyword_density.tsv', sep = '\\t')\n",
    "keyword_train_text = pd.read_csv('text_keyword_density.tsv', sep = '\\t')\n",
    "\n",
    "keyword_test_title = pd.read_csv('test_title_keyword_density.tsv', sep = '\\t')\n",
    "keyword_test_text = pd.read_csv('test_text_keyword_density.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>2733</td>\n",
       "      <td>257</td>\n",
       "      <td>16</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.536779</td>\n",
       "      <td>1.988072</td>\n",
       "      <td>2.584493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "      <td>2630</td>\n",
       "      <td>271</td>\n",
       "      <td>14</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.9197</td>\n",
       "      <td>0</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.384458</td>\n",
       "      <td>2.862986</td>\n",
       "      <td>2.658487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>4052</td>\n",
       "      <td>404</td>\n",
       "      <td>13</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.311902</td>\n",
       "      <td>1.778386</td>\n",
       "      <td>2.599179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>1131</td>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.9335</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.615385</td>\n",
       "      <td>2.403846</td>\n",
       "      <td>1.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>1061</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.4559</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>11.475410</td>\n",
       "      <td>2.732240</td>\n",
       "      <td>2.185792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0            1                67                 8             2733   \n",
       "1            2               121                14             2630   \n",
       "2            1                64                 7             4052   \n",
       "3            3                72                 7             1131   \n",
       "4            4               104                10             1061   \n",
       "\n",
       "   text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0              257                   16  0.109  0.789  0.102   -0.5574   \n",
       "1              271                   14  0.095  0.832  0.073   -0.9197   \n",
       "2              404                   13  0.052  0.859  0.089    0.9826   \n",
       "3              107                    5  0.022  0.884  0.094    0.9335   \n",
       "4              100                    7  0.077  0.836  0.087    0.4559   \n",
       "\n",
       "   Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0          1   10.000000          0.0    0.000000  10.536779  1.988072   \n",
       "1          0    4.347826          0.0    0.000000   8.384458  2.862986   \n",
       "2          1   16.666667          0.0    0.000000  12.311902  1.778386   \n",
       "3          0   20.000000          0.0    0.000000   9.615385  2.403846   \n",
       "4          1    6.666667          0.0    6.666667  11.475410  2.732240   \n",
       "\n",
       "    RB_text  \n",
       "0  2.584493  \n",
       "1  2.658487  \n",
       "2  2.599179  \n",
       "3  1.923077  \n",
       "4  2.185792  "
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([train_cat, train_tit_len, train_text_length, \n",
    "                       vader_train_outputs_text, vader_train_outputs_title, punctuated_train_df, keyword_train_title, keyword_train_text], axis=1)\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "merged_df.drop(columns=[\"id\", \"Unnamed: 0\"],inplace=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "# print(len(train_df['label']), len(merged_df))\n",
    "# train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(merged_df, train_df['label'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>2655</td>\n",
       "      <td>252</td>\n",
       "      <td>17</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.9787</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.178862</td>\n",
       "      <td>3.861789</td>\n",
       "      <td>3.252033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>2474</td>\n",
       "      <td>259</td>\n",
       "      <td>12</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>1</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>12.951168</td>\n",
       "      <td>1.698514</td>\n",
       "      <td>2.547771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>179</td>\n",
       "      <td>18</td>\n",
       "      <td>1394</td>\n",
       "      <td>128</td>\n",
       "      <td>15</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.9129</td>\n",
       "      <td>1</td>\n",
       "      <td>12.820513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>6.451613</td>\n",
       "      <td>1.433692</td>\n",
       "      <td>5.376344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>1434</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>1</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.323944</td>\n",
       "      <td>1.408451</td>\n",
       "      <td>2.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>1561</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>1</td>\n",
       "      <td>27.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.120567</td>\n",
       "      <td>2.836879</td>\n",
       "      <td>1.418440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>2248</td>\n",
       "      <td>225</td>\n",
       "      <td>16</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.9964</td>\n",
       "      <td>1</td>\n",
       "      <td>15.789474</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.751152</td>\n",
       "      <td>3.456221</td>\n",
       "      <td>3.917051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>1670</td>\n",
       "      <td>165</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.9136</td>\n",
       "      <td>1</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.564626</td>\n",
       "      <td>1.360544</td>\n",
       "      <td>1.700680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>3113</td>\n",
       "      <td>302</td>\n",
       "      <td>18</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>9.407666</td>\n",
       "      <td>1.916376</td>\n",
       "      <td>2.090592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>1604</td>\n",
       "      <td>147</td>\n",
       "      <td>8</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.8294</td>\n",
       "      <td>1</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.101010</td>\n",
       "      <td>3.367003</td>\n",
       "      <td>5.050505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0                1                51                 7             2655   \n",
       "1                1                63                10             2474   \n",
       "2                3               179                18             1394   \n",
       "3                1                52                 5             1434   \n",
       "4                5                68                10             1561   \n",
       "...            ...               ...               ...              ...   \n",
       "23995            6                88                10             2248   \n",
       "23996            5                77                 9             1670   \n",
       "23997            5                65                10             3113   \n",
       "23998            3                82                 8              150   \n",
       "23999            6                86                 9             1604   \n",
       "\n",
       "       text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0                  252                   17  0.102  0.853  0.045   -0.9787   \n",
       "1                  259                   12  0.038  0.830  0.132    0.9886   \n",
       "2                  128                   15  0.031  0.895  0.075    0.9129   \n",
       "3                  139                   12  0.016  0.773  0.211    0.9942   \n",
       "4                  162                   10  0.079  0.820  0.101    0.6478   \n",
       "...                ...                  ...    ...    ...    ...       ...   \n",
       "23995              225                   16  0.197  0.737  0.066   -0.9964   \n",
       "23996              165                   11  0.066  0.921  0.013   -0.9136   \n",
       "23997              302                   18  0.059  0.802  0.140    0.9940   \n",
       "23998               15                    2  0.164  0.747  0.089   -0.3818   \n",
       "23999              147                    8  0.125  0.775  0.100   -0.8294   \n",
       "\n",
       "       Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0              1    0.000000    11.111111   11.111111  11.178862  3.861789   \n",
       "1              1    9.090909     0.000000   18.181818  12.951168  1.698514   \n",
       "2              1   12.820513     0.000000    7.692308   6.451613  1.433692   \n",
       "3              1   28.571429     0.000000    0.000000  12.323944  1.408451   \n",
       "4              1   27.272727     0.000000    0.000000  13.120567  2.836879   \n",
       "...          ...         ...          ...         ...        ...       ...   \n",
       "23995          1   15.789474     5.263158    0.000000  11.751152  3.456221   \n",
       "23996          1    8.333333     0.000000    0.000000  11.564626  1.360544   \n",
       "23997          1   33.333333     0.000000   16.666667   9.407666  1.916376   \n",
       "23998          1    0.000000     0.000000    0.000000   6.250000  3.125000   \n",
       "23999          1   11.111111     0.000000    0.000000  10.101010  3.367003   \n",
       "\n",
       "        RB_text  \n",
       "0      3.252033  \n",
       "1      2.547771  \n",
       "2      5.376344  \n",
       "3      2.112676  \n",
       "4      1.418440  \n",
       "...         ...  \n",
       "23995  3.917051  \n",
       "23996  1.700680  \n",
       "23997  2.090592  \n",
       "23998  6.250000  \n",
       "23999  5.050505  \n",
       "\n",
       "[24000 rows x 17 columns]"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype = torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"gpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNetwork(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Tanh()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): Tanh()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): Tanh()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (10): Tanh()\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (13): Tanh()\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "\n",
    "        # More dense 1D Convolutional layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),  # (64, 17)\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (64, 8)\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),  # (128, 8)\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (128, 4)\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),  # (256, 4)\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (256, 2)\n",
    "\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),  # (512, 2)\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (512, 1)\n",
    "\n",
    "            nn.Conv1d(512, 1024, kernel_size=3, stride=1, padding=1),  # (1024, 1)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after the convolutional layers\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(1024, 512),  # Flattened input to the fully connected layer (1024 channels)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 17)  # Reshape input to [batch_size, 1, 17]\n",
    "        x = self.conv_layer(x)  # Apply convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor before feeding it into the fully connected layers\n",
    "        x = self.fc_layer(x)  # Apply fully connected layers\n",
    "        return x\n",
    "    \n",
    "model = ConvolutionalNetwork().to(device)\n",
    "print(model)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.701557  [   32/24000]\n",
      "loss: 0.525099  [ 3232/24000]\n",
      "loss: 0.508524  [ 6432/24000]\n",
      "loss: 0.192645  [ 9632/24000]\n",
      "loss: 0.245275  [12832/24000]\n",
      "loss: 0.154072  [16032/24000]\n",
      "loss: 0.280950  [19232/24000]\n",
      "loss: 0.432745  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.216232 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.308943  [   32/24000]\n",
      "loss: 0.171320  [ 3232/24000]\n",
      "loss: 0.121604  [ 6432/24000]\n",
      "loss: 0.172207  [ 9632/24000]\n",
      "loss: 0.185478  [12832/24000]\n",
      "loss: 0.042874  [16032/24000]\n",
      "loss: 0.297377  [19232/24000]\n",
      "loss: 0.192013  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.181236 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.090240  [   32/24000]\n",
      "loss: 0.102708  [ 3232/24000]\n",
      "loss: 0.339298  [ 6432/24000]\n",
      "loss: 0.235710  [ 9632/24000]\n",
      "loss: 0.328236  [12832/24000]\n",
      "loss: 0.329545  [16032/24000]\n",
      "loss: 0.231156  [19232/24000]\n",
      "loss: 0.148640  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.176117 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.132347  [   32/24000]\n",
      "loss: 0.141625  [ 3232/24000]\n",
      "loss: 0.293528  [ 6432/24000]\n",
      "loss: 0.099170  [ 9632/24000]\n",
      "loss: 0.117424  [12832/24000]\n",
      "loss: 0.366232  [16032/24000]\n",
      "loss: 0.269282  [19232/24000]\n",
      "loss: 0.203885  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.170198 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.543179  [   32/24000]\n",
      "loss: 0.171494  [ 3232/24000]\n",
      "loss: 0.261065  [ 6432/24000]\n",
      "loss: 0.153466  [ 9632/24000]\n",
      "loss: 0.083208  [12832/24000]\n",
      "loss: 0.149373  [16032/24000]\n",
      "loss: 0.132578  [19232/24000]\n",
      "loss: 0.212073  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.167510 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.034914  [   32/24000]\n",
      "loss: 0.130875  [ 3232/24000]\n",
      "loss: 0.068056  [ 6432/24000]\n",
      "loss: 0.029276  [ 9632/24000]\n",
      "loss: 0.038709  [12832/24000]\n",
      "loss: 0.037347  [16032/24000]\n",
      "loss: 0.078761  [19232/24000]\n",
      "loss: 0.040822  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.185404 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.083361  [   32/24000]\n",
      "loss: 0.091673  [ 3232/24000]\n",
      "loss: 0.289117  [ 6432/24000]\n",
      "loss: 0.090656  [ 9632/24000]\n",
      "loss: 0.080429  [12832/24000]\n",
      "loss: 0.286497  [16032/24000]\n",
      "loss: 0.061359  [19232/24000]\n",
      "loss: 0.376402  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.133817 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.095044  [   32/24000]\n",
      "loss: 0.163175  [ 3232/24000]\n",
      "loss: 0.201682  [ 6432/24000]\n",
      "loss: 0.034942  [ 9632/24000]\n",
      "loss: 0.101475  [12832/24000]\n",
      "loss: 0.203257  [16032/24000]\n",
      "loss: 0.019670  [19232/24000]\n",
      "loss: 0.082035  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.130812 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.169851  [   32/24000]\n",
      "loss: 0.114047  [ 3232/24000]\n",
      "loss: 0.111137  [ 6432/24000]\n",
      "loss: 0.268269  [ 9632/24000]\n",
      "loss: 0.026050  [12832/24000]\n",
      "loss: 0.216677  [16032/24000]\n",
      "loss: 0.282072  [19232/24000]\n",
      "loss: 0.095715  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.127082 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.094627  [   32/24000]\n",
      "loss: 0.133013  [ 3232/24000]\n",
      "loss: 0.046712  [ 6432/24000]\n",
      "loss: 0.063864  [ 9632/24000]\n",
      "loss: 0.102350  [12832/24000]\n",
      "loss: 0.091674  [16032/24000]\n",
      "loss: 0.139016  [19232/24000]\n",
      "loss: 0.104806  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.121070 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.039348  [   32/24000]\n",
      "loss: 0.189140  [ 3232/24000]\n",
      "loss: 0.106555  [ 6432/24000]\n",
      "loss: 0.323260  [ 9632/24000]\n",
      "loss: 0.025078  [12832/24000]\n",
      "loss: 0.053877  [16032/24000]\n",
      "loss: 0.056345  [19232/24000]\n",
      "loss: 0.069612  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.108246 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.033277  [   32/24000]\n",
      "loss: 0.075013  [ 3232/24000]\n",
      "loss: 0.226877  [ 6432/24000]\n",
      "loss: 0.275029  [ 9632/24000]\n",
      "loss: 0.025484  [12832/24000]\n",
      "loss: 0.007586  [16032/24000]\n",
      "loss: 0.049457  [19232/24000]\n",
      "loss: 0.118350  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.104989 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.098433  [   32/24000]\n",
      "loss: 0.041469  [ 3232/24000]\n",
      "loss: 0.095514  [ 6432/24000]\n",
      "loss: 0.031024  [ 9632/24000]\n",
      "loss: 0.014204  [12832/24000]\n",
      "loss: 0.078658  [16032/24000]\n",
      "loss: 0.106436  [19232/24000]\n",
      "loss: 0.160590  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.100081 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.217802  [   32/24000]\n",
      "loss: 0.062022  [ 3232/24000]\n",
      "loss: 0.045795  [ 6432/24000]\n",
      "loss: 0.080419  [ 9632/24000]\n",
      "loss: 0.056260  [12832/24000]\n",
      "loss: 0.064503  [16032/24000]\n",
      "loss: 0.065657  [19232/24000]\n",
      "loss: 0.055244  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.087070 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.104880  [   32/24000]\n",
      "loss: 0.034729  [ 3232/24000]\n",
      "loss: 0.188295  [ 6432/24000]\n",
      "loss: 0.028428  [ 9632/24000]\n",
      "loss: 0.142150  [12832/24000]\n",
      "loss: 0.076414  [16032/24000]\n",
      "loss: 0.044998  [19232/24000]\n",
      "loss: 0.243511  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.105193 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.017449  [   32/24000]\n",
      "loss: 0.112296  [ 3232/24000]\n",
      "loss: 0.028432  [ 6432/24000]\n",
      "loss: 0.086923  [ 9632/24000]\n",
      "loss: 0.027597  [12832/24000]\n",
      "loss: 0.005183  [16032/24000]\n",
      "loss: 0.031238  [19232/24000]\n",
      "loss: 0.006697  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.079323 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.024369  [   32/24000]\n",
      "loss: 0.029238  [ 3232/24000]\n",
      "loss: 0.039072  [ 6432/24000]\n",
      "loss: 0.082062  [ 9632/24000]\n",
      "loss: 0.045622  [12832/24000]\n",
      "loss: 0.019952  [16032/24000]\n",
      "loss: 0.024155  [19232/24000]\n",
      "loss: 0.012816  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.063252 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.015381  [   32/24000]\n",
      "loss: 0.235789  [ 3232/24000]\n",
      "loss: 0.099703  [ 6432/24000]\n",
      "loss: 0.007045  [ 9632/24000]\n",
      "loss: 0.068458  [12832/24000]\n",
      "loss: 0.052233  [16032/24000]\n",
      "loss: 0.072468  [19232/24000]\n",
      "loss: 0.010097  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.055296 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.053921  [   32/24000]\n",
      "loss: 0.010086  [ 3232/24000]\n",
      "loss: 0.009158  [ 6432/24000]\n",
      "loss: 0.012502  [ 9632/24000]\n",
      "loss: 0.011406  [12832/24000]\n",
      "loss: 0.022344  [16032/24000]\n",
      "loss: 0.230964  [19232/24000]\n",
      "loss: 0.018205  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.047100 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.003682  [   32/24000]\n",
      "loss: 0.018312  [ 3232/24000]\n",
      "loss: 0.003856  [ 6432/24000]\n",
      "loss: 0.004392  [ 9632/24000]\n",
      "loss: 0.003478  [12832/24000]\n",
      "loss: 0.011208  [16032/24000]\n",
      "loss: 0.029479  [19232/24000]\n",
      "loss: 0.004438  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.037138 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.020396  [   32/24000]\n",
      "loss: 0.019824  [ 3232/24000]\n",
      "loss: 0.002396  [ 6432/24000]\n",
      "loss: 0.065629  [ 9632/24000]\n",
      "loss: 0.017695  [12832/24000]\n",
      "loss: 0.018618  [16032/24000]\n",
      "loss: 0.003524  [19232/24000]\n",
      "loss: 0.002559  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.041378 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.020729  [   32/24000]\n",
      "loss: 0.001835  [ 3232/24000]\n",
      "loss: 0.003269  [ 6432/24000]\n",
      "loss: 0.004226  [ 9632/24000]\n",
      "loss: 0.016963  [12832/24000]\n",
      "loss: 0.007450  [16032/24000]\n",
      "loss: 0.004193  [19232/24000]\n",
      "loss: 0.004648  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.031514 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.053480  [   32/24000]\n",
      "loss: 0.002937  [ 3232/24000]\n",
      "loss: 0.002186  [ 6432/24000]\n",
      "loss: 0.005671  [ 9632/24000]\n",
      "loss: 0.004558  [12832/24000]\n",
      "loss: 0.005916  [16032/24000]\n",
      "loss: 0.003652  [19232/24000]\n",
      "loss: 0.002025  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.021246 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.012134  [   32/24000]\n",
      "loss: 0.001843  [ 3232/24000]\n",
      "loss: 0.002036  [ 6432/24000]\n",
      "loss: 0.001517  [ 9632/24000]\n",
      "loss: 0.002409  [12832/24000]\n",
      "loss: 0.035974  [16032/24000]\n",
      "loss: 0.001678  [19232/24000]\n",
      "loss: 0.002426  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.020251 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.001435  [   32/24000]\n",
      "loss: 0.001225  [ 3232/24000]\n",
      "loss: 0.000978  [ 6432/24000]\n",
      "loss: 0.000725  [ 9632/24000]\n",
      "loss: 0.003295  [12832/24000]\n",
      "loss: 0.010846  [16032/24000]\n",
      "loss: 0.016168  [19232/24000]\n",
      "loss: 0.000442  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.018130 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.001073  [   32/24000]\n",
      "loss: 0.016626  [ 3232/24000]\n",
      "loss: 0.003303  [ 6432/24000]\n",
      "loss: 0.001255  [ 9632/24000]\n",
      "loss: 0.002843  [12832/24000]\n",
      "loss: 0.004897  [16032/24000]\n",
      "loss: 0.003020  [19232/24000]\n",
      "loss: 0.001241  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.015543 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.002799  [   32/24000]\n",
      "loss: 0.001122  [ 3232/24000]\n",
      "loss: 0.000453  [ 6432/24000]\n",
      "loss: 0.000620  [ 9632/24000]\n",
      "loss: 0.004406  [12832/24000]\n",
      "loss: 0.000722  [16032/24000]\n",
      "loss: 0.000236  [19232/24000]\n",
      "loss: 0.009725  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.016251 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.000622  [   32/24000]\n",
      "loss: 0.000356  [ 3232/24000]\n",
      "loss: 0.010925  [ 6432/24000]\n",
      "loss: 0.002362  [ 9632/24000]\n",
      "loss: 0.002004  [12832/24000]\n",
      "loss: 0.000767  [16032/24000]\n",
      "loss: 0.000433  [19232/24000]\n",
      "loss: 0.000468  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.015471 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000242  [   32/24000]\n",
      "loss: 0.000498  [ 3232/24000]\n",
      "loss: 0.001268  [ 6432/24000]\n",
      "loss: 0.000379  [ 9632/24000]\n",
      "loss: 0.000536  [12832/24000]\n",
      "loss: 0.000456  [16032/24000]\n",
      "loss: 0.000563  [19232/24000]\n",
      "loss: 0.001990  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.015187 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.001003  [   32/24000]\n",
      "loss: 0.000327  [ 3232/24000]\n",
      "loss: 0.000450  [ 6432/24000]\n",
      "loss: 0.000217  [ 9632/24000]\n",
      "loss: 0.000503  [12832/24000]\n",
      "loss: 0.000398  [16032/24000]\n",
      "loss: 0.003773  [19232/24000]\n",
      "loss: 0.000869  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.015073 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000248  [   32/24000]\n",
      "loss: 0.001521  [ 3232/24000]\n",
      "loss: 0.001112  [ 6432/24000]\n",
      "loss: 0.000484  [ 9632/24000]\n",
      "loss: 0.000118  [12832/24000]\n",
      "loss: 0.001197  [16032/24000]\n",
      "loss: 0.000469  [19232/24000]\n",
      "loss: 0.000196  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.013756 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.000722  [   32/24000]\n",
      "loss: 0.000144  [ 3232/24000]\n",
      "loss: 0.000624  [ 6432/24000]\n",
      "loss: 0.000625  [ 9632/24000]\n",
      "loss: 0.001061  [12832/24000]\n",
      "loss: 0.001110  [16032/24000]\n",
      "loss: 0.001483  [19232/24000]\n",
      "loss: 0.000130  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.013784 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.000296  [   32/24000]\n",
      "loss: 0.000237  [ 3232/24000]\n",
      "loss: 0.000454  [ 6432/24000]\n",
      "loss: 0.000788  [ 9632/24000]\n",
      "loss: 0.001175  [12832/24000]\n",
      "loss: 0.000138  [16032/24000]\n",
      "loss: 0.000950  [19232/24000]\n",
      "loss: 0.000375  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.013851 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000161  [   32/24000]\n",
      "loss: 0.000450  [ 3232/24000]\n",
      "loss: 0.000322  [ 6432/24000]\n",
      "loss: 0.000573  [ 9632/24000]\n",
      "loss: 0.000410  [12832/24000]\n",
      "loss: 0.000254  [16032/24000]\n",
      "loss: 0.000387  [19232/24000]\n",
      "loss: 0.000565  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012996 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000335  [   32/24000]\n",
      "loss: 0.000296  [ 3232/24000]\n",
      "loss: 0.000230  [ 6432/24000]\n",
      "loss: 0.000914  [ 9632/24000]\n",
      "loss: 0.001993  [12832/24000]\n",
      "loss: 0.000269  [16032/24000]\n",
      "loss: 0.000159  [19232/24000]\n",
      "loss: 0.000059  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.013323 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.001045  [   32/24000]\n",
      "loss: 0.000240  [ 3232/24000]\n",
      "loss: 0.000428  [ 6432/24000]\n",
      "loss: 0.000616  [ 9632/24000]\n",
      "loss: 0.000279  [12832/24000]\n",
      "loss: 0.000285  [16032/24000]\n",
      "loss: 0.001352  [19232/24000]\n",
      "loss: 0.000191  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013201 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.000253  [   32/24000]\n",
      "loss: 0.001313  [ 3232/24000]\n",
      "loss: 0.000288  [ 6432/24000]\n",
      "loss: 0.000222  [ 9632/24000]\n",
      "loss: 0.000063  [12832/24000]\n",
      "loss: 0.000087  [16032/24000]\n",
      "loss: 0.000768  [19232/24000]\n",
      "loss: 0.000177  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.013629 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000313  [   32/24000]\n",
      "loss: 0.000637  [ 3232/24000]\n",
      "loss: 0.000064  [ 6432/24000]\n",
      "loss: 0.000382  [ 9632/24000]\n",
      "loss: 0.000201  [12832/24000]\n",
      "loss: 0.000824  [16032/24000]\n",
      "loss: 0.000469  [19232/24000]\n",
      "loss: 0.000143  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012982 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000223  [   32/24000]\n",
      "loss: 0.000172  [ 3232/24000]\n",
      "loss: 0.000487  [ 6432/24000]\n",
      "loss: 0.000581  [ 9632/24000]\n",
      "loss: 0.000891  [12832/24000]\n",
      "loss: 0.000925  [16032/24000]\n",
      "loss: 0.000502  [19232/24000]\n",
      "loss: 0.000252  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013104 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000396  [   32/24000]\n",
      "loss: 0.000253  [ 3232/24000]\n",
      "loss: 0.000100  [ 6432/24000]\n",
      "loss: 0.000075  [ 9632/24000]\n",
      "loss: 0.000467  [12832/24000]\n",
      "loss: 0.000235  [16032/24000]\n",
      "loss: 0.000470  [19232/24000]\n",
      "loss: 0.000217  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013242 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000369  [   32/24000]\n",
      "loss: 0.000102  [ 3232/24000]\n",
      "loss: 0.000258  [ 6432/24000]\n",
      "loss: 0.000228  [ 9632/24000]\n",
      "loss: 0.000155  [12832/24000]\n",
      "loss: 0.000143  [16032/24000]\n",
      "loss: 0.000340  [19232/24000]\n",
      "loss: 0.000226  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013006 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000134  [   32/24000]\n",
      "loss: 0.000562  [ 3232/24000]\n",
      "loss: 0.000083  [ 6432/24000]\n",
      "loss: 0.000038  [ 9632/24000]\n",
      "loss: 0.000068  [12832/24000]\n",
      "loss: 0.000305  [16032/24000]\n",
      "loss: 0.000124  [19232/24000]\n",
      "loss: 0.000323  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.012972 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000083  [   32/24000]\n",
      "loss: 0.001464  [ 3232/24000]\n",
      "loss: 0.000122  [ 6432/24000]\n",
      "loss: 0.000249  [ 9632/24000]\n",
      "loss: 0.000170  [12832/24000]\n",
      "loss: 0.000297  [16032/24000]\n",
      "loss: 0.000204  [19232/24000]\n",
      "loss: 0.000756  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013030 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.000581  [   32/24000]\n",
      "loss: 0.000106  [ 3232/24000]\n",
      "loss: 0.000163  [ 6432/24000]\n",
      "loss: 0.000064  [ 9632/24000]\n",
      "loss: 0.000170  [12832/24000]\n",
      "loss: 0.000157  [16032/24000]\n",
      "loss: 0.000183  [19232/24000]\n",
      "loss: 0.000078  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012847 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000260  [   32/24000]\n",
      "loss: 0.000083  [ 3232/24000]\n",
      "loss: 0.000281  [ 6432/24000]\n",
      "loss: 0.000262  [ 9632/24000]\n",
      "loss: 0.000525  [12832/24000]\n",
      "loss: 0.000406  [16032/24000]\n",
      "loss: 0.000094  [19232/24000]\n",
      "loss: 0.001740  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012948 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000145  [   32/24000]\n",
      "loss: 0.000144  [ 3232/24000]\n",
      "loss: 0.000910  [ 6432/24000]\n",
      "loss: 0.000301  [ 9632/24000]\n",
      "loss: 0.000066  [12832/24000]\n",
      "loss: 0.000162  [16032/24000]\n",
      "loss: 0.000124  [19232/24000]\n",
      "loss: 0.000152  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012992 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000094  [   32/24000]\n",
      "loss: 0.000257  [ 3232/24000]\n",
      "loss: 0.000207  [ 6432/24000]\n",
      "loss: 0.000031  [ 9632/24000]\n",
      "loss: 0.000188  [12832/24000]\n",
      "loss: 0.000080  [16032/24000]\n",
      "loss: 0.000162  [19232/24000]\n",
      "loss: 0.000131  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013019 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000202  [   32/24000]\n",
      "loss: 0.000143  [ 3232/24000]\n",
      "loss: 0.000142  [ 6432/24000]\n",
      "loss: 0.000063  [ 9632/24000]\n",
      "loss: 0.000202  [12832/24000]\n",
      "loss: 0.000305  [16032/24000]\n",
      "loss: 0.000139  [19232/24000]\n",
      "loss: 0.000172  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.013018 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.000052  [   32/24000]\n",
      "loss: 0.000594  [ 3232/24000]\n",
      "loss: 0.000122  [ 6432/24000]\n",
      "loss: 0.000147  [ 9632/24000]\n",
      "loss: 0.000038  [12832/24000]\n",
      "loss: 0.000667  [16032/24000]\n",
      "loss: 0.000116  [19232/24000]\n",
      "loss: 0.000086  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012838 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000085  [   32/24000]\n",
      "loss: 0.000139  [ 3232/24000]\n",
      "loss: 0.000572  [ 6432/24000]\n",
      "loss: 0.000268  [ 9632/24000]\n",
      "loss: 0.000403  [12832/24000]\n",
      "loss: 0.000169  [16032/24000]\n",
      "loss: 0.000109  [19232/24000]\n",
      "loss: 0.000078  [22432/24000]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 0.012964 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") \n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "23995    0\n",
       "23996    1\n",
       "23997    1\n",
       "23998    0\n",
       "23999    0\n",
       "Name: label, Length: 24000, dtype: int64"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>2655</td>\n",
       "      <td>252</td>\n",
       "      <td>17</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.9787</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.178862</td>\n",
       "      <td>3.861789</td>\n",
       "      <td>3.252033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>2474</td>\n",
       "      <td>259</td>\n",
       "      <td>12</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>1</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>12.951168</td>\n",
       "      <td>1.698514</td>\n",
       "      <td>2.547771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>179</td>\n",
       "      <td>18</td>\n",
       "      <td>1394</td>\n",
       "      <td>128</td>\n",
       "      <td>15</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.9129</td>\n",
       "      <td>1</td>\n",
       "      <td>12.820513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>6.451613</td>\n",
       "      <td>1.433692</td>\n",
       "      <td>5.376344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>1434</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>1</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.323944</td>\n",
       "      <td>1.408451</td>\n",
       "      <td>2.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>1561</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>1</td>\n",
       "      <td>27.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.120567</td>\n",
       "      <td>2.836879</td>\n",
       "      <td>1.418440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>2248</td>\n",
       "      <td>225</td>\n",
       "      <td>16</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.9964</td>\n",
       "      <td>1</td>\n",
       "      <td>15.789474</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.751152</td>\n",
       "      <td>3.456221</td>\n",
       "      <td>3.917051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>1670</td>\n",
       "      <td>165</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.9136</td>\n",
       "      <td>1</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.564626</td>\n",
       "      <td>1.360544</td>\n",
       "      <td>1.700680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>3113</td>\n",
       "      <td>302</td>\n",
       "      <td>18</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>9.407666</td>\n",
       "      <td>1.916376</td>\n",
       "      <td>2.090592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>6.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>1604</td>\n",
       "      <td>147</td>\n",
       "      <td>8</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.8294</td>\n",
       "      <td>1</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.101010</td>\n",
       "      <td>3.367003</td>\n",
       "      <td>5.050505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0                1                51                 7             2655   \n",
       "1                1                63                10             2474   \n",
       "2                3               179                18             1394   \n",
       "3                1                52                 5             1434   \n",
       "4                5                68                10             1561   \n",
       "...            ...               ...               ...              ...   \n",
       "23995            6                88                10             2248   \n",
       "23996            5                77                 9             1670   \n",
       "23997            5                65                10             3113   \n",
       "23998            3                82                 8              150   \n",
       "23999            6                86                 9             1604   \n",
       "\n",
       "       text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0                  252                   17  0.102  0.853  0.045   -0.9787   \n",
       "1                  259                   12  0.038  0.830  0.132    0.9886   \n",
       "2                  128                   15  0.031  0.895  0.075    0.9129   \n",
       "3                  139                   12  0.016  0.773  0.211    0.9942   \n",
       "4                  162                   10  0.079  0.820  0.101    0.6478   \n",
       "...                ...                  ...    ...    ...    ...       ...   \n",
       "23995              225                   16  0.197  0.737  0.066   -0.9964   \n",
       "23996              165                   11  0.066  0.921  0.013   -0.9136   \n",
       "23997              302                   18  0.059  0.802  0.140    0.9940   \n",
       "23998               15                    2  0.164  0.747  0.089   -0.3818   \n",
       "23999              147                    8  0.125  0.775  0.100   -0.8294   \n",
       "\n",
       "       Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0              1    0.000000    11.111111   11.111111  11.178862  3.861789   \n",
       "1              1    9.090909     0.000000   18.181818  12.951168  1.698514   \n",
       "2              1   12.820513     0.000000    7.692308   6.451613  1.433692   \n",
       "3              1   28.571429     0.000000    0.000000  12.323944  1.408451   \n",
       "4              1   27.272727     0.000000    0.000000  13.120567  2.836879   \n",
       "...          ...         ...          ...         ...        ...       ...   \n",
       "23995          1   15.789474     5.263158    0.000000  11.751152  3.456221   \n",
       "23996          1    8.333333     0.000000    0.000000  11.564626  1.360544   \n",
       "23997          1   33.333333     0.000000   16.666667   9.407666  1.916376   \n",
       "23998          1    0.000000     0.000000    0.000000   6.250000  3.125000   \n",
       "23999          1   11.111111     0.000000    0.000000  10.101010  3.367003   \n",
       "\n",
       "        RB_text  \n",
       "0      3.252033  \n",
       "1      2.547771  \n",
       "2      5.376344  \n",
       "3      2.112676  \n",
       "4      1.418440  \n",
       "...         ...  \n",
       "23995  3.917051  \n",
       "23996  1.700680  \n",
       "23997  2.090592  \n",
       "23998  6.250000  \n",
       "23999  5.050505  \n",
       "\n",
       "[24000 rows x 17 columns]"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): Tanh()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): Tanh()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): Tanh()\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (10): Tanh()\n",
       "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (13): Tanh()\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():  # Disable gradient calculation\n",
    "#     outputs = model(X_test_tensor)  # Get logits\n",
    "#     probabilities = torch.sigmoid(outputs)  # Apply sigmoid for probabilities\n",
    "#     predictions = (probabilities > 0.5).float()  # Convert to 0 or 1\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(X_test_tensor)  # Get logits (size: [batch_size, 2])\n",
    "    probabilities = torch.softmax(outputs, dim=1)  # Apply softmax to get class probabilities\n",
    "    predictions = torch.argmax(probabilities, dim=1)  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 6000\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.cpu().numpy()\n",
    "y_true = y_test_tensor.view(-1).cpu().numpy()\n",
    "print(len(y_pred), len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.78%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_pred == y_true).sum() / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8104</td>\n",
       "      <td>Conservatives Will HATE What Donald Trump Just...</td>\n",
       "      <td>Donald Trump isn t exactly a stranger to makin...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7467</td>\n",
       "      <td>Trump victory may create new tension between U...</td>\n",
       "      <td>Donald Trump’s U.S. election victory may creat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9473</td>\n",
       "      <td>WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...</td>\n",
       "      <td>A couple of quick questions come to mind when ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 9, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276</td>\n",
       "      <td>Democratic Senator Franken to resign: CNN, cit...</td>\n",
       "      <td>U.S. Democratic Senator Al Franken will announ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19274</td>\n",
       "      <td>GANG OF DOMESTIC TERRORISTS Violently Attack L...</td>\n",
       "      <td>***WARNING*** Violence is graphic***This Trump...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jan 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8104  Conservatives Will HATE What Donald Trump Just...   \n",
       "1        7467  Trump victory may create new tension between U...   \n",
       "2        9473  WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...   \n",
       "3         276  Democratic Senator Franken to resign: CNN, cit...   \n",
       "4       19274  GANG OF DOMESTIC TERRORISTS Violently Attack L...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump isn t exactly a stranger to makin...          News   \n",
       "1  Donald Trump’s U.S. election victory may creat...  politicsNews   \n",
       "2  A couple of quick questions come to mind when ...      politics   \n",
       "3  U.S. Democratic Senator Al Franken will announ...  politicsNews   \n",
       "4  ***WARNING*** Violence is graphic***This Trump...     left-news   \n",
       "\n",
       "                date  label  \n",
       "0  February 14, 2016      0  \n",
       "1  November 9, 2016       1  \n",
       "2        Nov 9, 2017      0  \n",
       "3  December 7, 2017       1  \n",
       "4       Jan 21, 2017      0  "
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns = \"Unnamed: 0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conservatives Will HATE What Donald Trump Just...</td>\n",
       "      <td>Donald Trump isn t exactly a stranger to makin...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump victory may create new tension between U...</td>\n",
       "      <td>Donald Trump’s U.S. election victory may creat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...</td>\n",
       "      <td>A couple of quick questions come to mind when ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 9, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic Senator Franken to resign: CNN, cit...</td>\n",
       "      <td>U.S. Democratic Senator Al Franken will announ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GANG OF DOMESTIC TERRORISTS Violently Attack L...</td>\n",
       "      <td>***WARNING*** Violence is graphic***This Trump...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jan 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Conservatives Will HATE What Donald Trump Just...   \n",
       "1  Trump victory may create new tension between U...   \n",
       "2  WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...   \n",
       "3  Democratic Senator Franken to resign: CNN, cit...   \n",
       "4  GANG OF DOMESTIC TERRORISTS Violently Attack L...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump isn t exactly a stranger to makin...          News   \n",
       "1  Donald Trump’s U.S. election victory may creat...  politicsNews   \n",
       "2  A couple of quick questions come to mind when ...      politics   \n",
       "3  U.S. Democratic Senator Al Franken will announ...  politicsNews   \n",
       "4  ***WARNING*** Violence is graphic***This Trump...     left-news   \n",
       "\n",
       "                date  label  \n",
       "0  February 14, 2016      0  \n",
       "1  November 9, 2016       1  \n",
       "2        Nov 9, 2017      0  \n",
       "3  December 7, 2017       1  \n",
       "4       Jan 21, 2017      0  "
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = pd.read_csv(\"test_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tit_len_test = pd.read_csv(\"test_title_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_length_test = pd.read_csv(\"test_text_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_train_outputs_text_test = pd.read_csv(\"vader_test_outputs_text.csv\")\n",
    "vader_train_outputs_title_test = pd.read_csv(\"vader_test_outputs_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Punctuate, dtype: int64"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuated_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Punctuate</th>\n",
       "      <th>JJ_density</th>\n",
       "      <th>VBG_density</th>\n",
       "      <th>RB_density</th>\n",
       "      <th>JJ_text</th>\n",
       "      <th>VBG_text</th>\n",
       "      <th>RB_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>2290</td>\n",
       "      <td>209</td>\n",
       "      <td>15</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>6.733167</td>\n",
       "      <td>2.743142</td>\n",
       "      <td>5.486284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>658</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>1</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.516129</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>2.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>11</td>\n",
       "      <td>2644</td>\n",
       "      <td>242</td>\n",
       "      <td>17</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.8980</td>\n",
       "      <td>1</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>10.267857</td>\n",
       "      <td>2.008929</td>\n",
       "      <td>6.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.638298</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>15</td>\n",
       "      <td>344</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.9206</td>\n",
       "      <td>1</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>9.677419</td>\n",
       "      <td>4.838710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  title_char_count  title_word_count  text_char_count  \\\n",
       "0            6                84                 8             2290   \n",
       "1            1                84                11              658   \n",
       "2            4               103                11             2644   \n",
       "3            1                57                 7              279   \n",
       "4            3               123                15              344   \n",
       "\n",
       "   text_word_count  text_sentence_count    neg    neu    pos  compound  \\\n",
       "0              209                   15  0.109  0.724  0.167    0.9849   \n",
       "1               70                    4  0.102  0.771  0.127    0.4215   \n",
       "2              242                   17  0.082  0.872  0.046   -0.8980   \n",
       "3               27                    1  0.050  0.950  0.000   -0.2960   \n",
       "4               26                    1  0.294  0.612  0.093   -0.9206   \n",
       "\n",
       "   Punctuate  JJ_density  VBG_density  RB_density    JJ_text  VBG_text  \\\n",
       "0          1    0.000000     0.000000    7.142857   6.733167  2.743142   \n",
       "1          1   21.428571     0.000000    0.000000  14.516129  2.419355   \n",
       "2          1   11.764706     5.882353    5.882353  10.267857  2.008929   \n",
       "3          1   10.000000    10.000000    0.000000  10.638298  4.255319   \n",
       "4          1   18.181818     4.545455    4.545455   9.677419  4.838710   \n",
       "\n",
       "    RB_text  \n",
       "0  5.486284  \n",
       "1  2.419355  \n",
       "2  6.250000  \n",
       "3  0.000000  \n",
       "4  0.000000  "
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test_df = pd.concat([test_cat, test_tit_len_test, train_text_length_test, \n",
    "                       vader_train_outputs_text_test, vader_train_outputs_title_test, punctuated_test_df, keyword_test_title, keyword_test_text], axis=1)\n",
    "merged_test_df = merged_test_df.loc[:, ~merged_test_df.columns.duplicated()]\n",
    "merged_test_df.drop(columns=[\"id\", \"Unnamed: 0\"],inplace=True)\n",
    "merged_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = merged_test_df.values\n",
    "y_test = test_df['label']\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(X_test_tensor)  # Get logits (size: [batch_size, 2])\n",
    "    probabilities = torch.softmax(outputs, dim=1)  # Apply softmax to get class probabilities\n",
    "    predictions = torch.argmax(probabilities, dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8267 8267\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.cpu().numpy()\n",
    "y_true = y_test_tensor.view(-1).cpu().numpy()\n",
    "print(len(y_pred), len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.46%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_pred == y_true).sum() / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAHWCAYAAAAW1aGcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU/dJREFUeJzt3XlcVGX7P/DPAM6AwLCobC6IQgiKmlo2YS6JoIK50JOkCS64BZbiFrnhFqYpbqSlJX5NSs3UEjcUl8fEJRVFTRTFMBU0FQhlE873Dx+mRhxnRgcOMp/393VeP+ec+9znOvPjiYvrPvd9JIIgCCAiIiJ6CiOxAyAiIqLqi4kCERERqcVEgYiIiNRiokBERERqMVEgIiIitZgoEBERkVpMFIiIiEgtJgpERESkFhMFIiIiUouJApGWLl++DF9fX1hZWUEikWDr1q167f/atWuQSCSIi4vTa78vs86dO6Nz585ih0Fk0Jgo0EvlypUrGDlyJJo0aQJTU1PI5XJ4e3tjyZIlKCgoqNRrh4SEIDU1FXPnzsW6devQrl27Sr1eVRo8eDAkEgnkcvlTv8fLly9DIpFAIpHgiy++0Ln/mzdvIioqCikpKXqIloiqkonYARBpKyEhAf/5z38gk8kQHByMFi1aoLi4GIcPH8bEiRNx/vx5fP3115Vy7YKCAiQnJ2PKlCkIDw+vlGs4OzujoKAAtWrVqpT+NTExMcHDhw/xyy+/4L333lM5tn79epiamqKwsPC5+r558yZmzpyJxo0bo3Xr1lqft2fPnue6HhHpDxMFeilkZGQgKCgIzs7OSEpKgqOjo/JYWFgY0tPTkZCQUGnXv3PnDgDA2tq60q4hkUhgampaaf1rIpPJ4O3tje+//75CohAfHw9/f39s3ry5SmJ5+PAhateuDalUWiXXIyL1OPRAL4X58+cjPz8f33zzjUqSUM7V1RUff/yx8vOjR48we/ZsNG3aFDKZDI0bN8ann36KoqIilfMaN26MgIAAHD58GK+//jpMTU3RpEkT/N///Z+yTVRUFJydnQEAEydOhEQiQePGjQE8LtmX//vfoqKiIJFIVPYlJiaiQ4cOsLa2hoWFBdzd3fHpp58qj6t7RiEpKQlvvfUWzM3NYW1tjd69e+P3339/6vXS09MxePBgWFtbw8rKCkOGDMHDhw/Vf7FPGDBgAHbu3ImcnBzlvhMnTuDy5csYMGBAhfb37t3DhAkT4OXlBQsLC8jlcvTo0QNnzpxRtjlw4ABee+01AMCQIUOUQxjl99m5c2e0aNECJ0+eRMeOHVG7dm3l9/LkMwohISEwNTWtcP9+fn6wsbHBzZs3tb5XItIOEwV6Kfzyyy9o0qQJ3nzzTa3ah4aGYvr06WjTpg1iYmLQqVMnREdHIygoqELb9PR0vPvuu+jWrRsWLlwIGxsbDB48GOfPnwcA9OvXDzExMQCA999/H+vWrcPixYt1iv/8+fMICAhAUVERZs2ahYULF+Kdd97Br7/++szz9u7dCz8/P9y+fRtRUVGIiIjAkSNH4O3tjWvXrlVo/9577+Hvv/9GdHQ03nvvPcTFxWHmzJlax9mvXz9IJBL89NNPyn3x8fFo1qwZ2rRpU6H91atXsXXrVgQEBGDRokWYOHEiUlNT0alTJ+UvbQ8PD8yaNQsAMGLECKxbtw7r1q1Dx44dlf3cvXsXPXr0QOvWrbF48WJ06dLlqfEtWbIE9erVQ0hICEpLSwEAX331Ffbs2YNly5bByclJ63slIi0JRNVcbm6uAEDo3bu3Vu1TUlIEAEJoaKjK/gkTJggAhKSkJOU+Z2dnAYBw6NAh5b7bt28LMplMGD9+vHJfRkaGAEBYsGCBSp8hISGCs7NzhRhmzJgh/Pt/XjExMQIA4c6dO2rjLr/GmjVrlPtat24t2NnZCXfv3lXuO3PmjGBkZCQEBwdXuN7QoUNV+uzbt69Qp04dtdf8932Ym5sLgiAI7777rtC1a1dBEAShtLRUcHBwEGbOnPnU76CwsFAoLS2tcB8ymUyYNWuWct+JEycq3Fu5Tp06CQCElStXPvVYp06dVPbt3r1bACDMmTNHuHr1qmBhYSH06dNH4z0S0fNhRYGqvby8PACApaWlVu137NgBAIiIiFDZP378eACo8CyDp6cn3nrrLeXnevXqwd3dHVevXn3umJ9U/mzDtm3bUFZWptU5t27dQkpKCgYPHgxbW1vl/pYtW6Jbt27K+/y3UaNGqXx+6623cPfuXeV3qI0BAwbgwIEDyMrKQlJSErKysp467AA8fq7ByOjxf0ZKS0tx9+5d5bDKqVOntL6mTCbDkCFDtGrr6+uLkSNHYtasWejXrx9MTU3x1VdfaX0tItINEwWq9uRyOQDg77//1qr9H3/8ASMjI7i6uqrsd3BwgLW1Nf744w+V/Y0aNarQh42NDe7fv/+cEVfUv39/eHt7IzQ0FPb29ggKCsLGjRufmTSUx+nu7l7hmIeHB/766y88ePBAZf+T92JjYwMAOt1Lz549YWlpiQ0bNmD9+vV47bXXKnyX5crKyhATEwM3NzfIZDLUrVsX9erVw9mzZ5Gbm6v1NevXr6/Tg4tffPEFbG1tkZKSgqVLl8LOzk7rc4lIN0wUqNqTy+VwcnLCuXPndDrvyYcJ1TE2Nn7qfkEQnvsa5ePn5czMzHDo0CHs3bsXgwYNwtmzZ9G/f39069atQtsX8SL3Uk4mk6Ffv35Yu3YttmzZoraaAACfffYZIiIi0LFjR3z33XfYvXs3EhMT0bx5c60rJ8Dj70cXp0+fxu3btwEAqampOp1LRLphokAvhYCAAFy5cgXJycka2zo7O6OsrAyXL19W2Z+dnY2cnBzlDAZ9sLGxUZkhUO7JqgUAGBkZoWvXrli0aBEuXLiAuXPnIikpCfv3739q3+VxpqWlVTh28eJF1K1bF+bm5i92A2oMGDAAp0+fxt9///3UB0DL/fjjj+jSpQu++eYbBAUFwdfXFz4+PhW+E22TNm08ePAAQ4YMgaenJ0aMGIH58+fjxIkTeuufiFQxUaCXwqRJk2Bubo7Q0FBkZ2dXOH7lyhUsWbIEwOPSOYAKMxMWLVoEAPD399dbXE2bNkVubi7Onj2r3Hfr1i1s2bJFpd29e/cqnFu+8NCTUzbLOTo6onXr1li7dq3KL95z585hz549yvusDF26dMHs2bOxfPlyODg4qG1nbGxcoVqxadMm3LhxQ2VfeULztKRKV5MnT0ZmZibWrl2LRYsWoXHjxggJCVH7PRLRi+GCS/RSaNq0KeLj49G/f394eHiorMx45MgRbNq0CYMHDwYAtGrVCiEhIfj666+Rk5ODTp064fjx41i7di369Omjdurd8wgKCsLkyZPRt29ffPTRR3j48CFWrFiBV155ReVhvlmzZuHQoUPw9/eHs7Mzbt++jS+//BINGjRAhw4d1Pa/YMEC9OjRAwqFAsOGDUNBQQGWLVsGKysrREVF6e0+nmRkZISpU6dqbBcQEIBZs2ZhyJAhePPNN5Gamor169ejSZMmKu2aNm0Ka2trrFy5EpaWljA3N0f79u3h4uKiU1xJSUn48ssvMWPGDOV0zTVr1qBz586YNm0a5s+fr1N/RKQFkWddEOnk0qVLwvDhw4XGjRsLUqlUsLS0FLy9vYVly5YJhYWFynYlJSXCzJkzBRcXF6FWrVpCw4YNhcjISJU2gvB4eqS/v3+F6zw5LU/d9EhBEIQ9e/YILVq0EKRSqeDu7i589913FaZH7tu3T+jdu7fg5OQkSKVSwcnJSXj//feFS5cuVbjGk1MI9+7dK3h7ewtmZmaCXC4XevXqJVy4cEGlTfn1npx+uWbNGgGAkJGRofY7FQTV6ZHqqJseOX78eMHR0VEwMzMTvL29heTk5KdOa9y2bZvg6ekpmJiYqNxnp06dhObNmz/1mv/uJy8vT3B2dhbatGkjlJSUqLQbN26cYGRkJCQnJz/zHohIdxJB0OEpJyIiIjIofEaBiIiI1GKiQERERGoxUSAiIiK1mCgQERGRWkwUiIiISC0mCkRERKQWEwUiIiJSq0auzGj2arjYIRBVuvsnlosdAlGlM63k31L6/H1RcLpm/m+yRiYKREREWpGwsK4JvyEiIiJSixUFIiIyXHp8BXpNxUSBiIgMF4ceNOI3RERERGqxokBERIaLQw8aMVEgIiLDxaEHjfgNERERkVqsKBARkeHi0INGTBSIiMhwcehBI35DREREpBYrCkREZLg49KAREwUiIjJcHHrQiN8QERERqcWKAhERGS4OPWjERIGIiAwXhx404jdEREREarGiQEREhotDDxoxUSAiIsPFoQeN+A0RERGRWqwoEBGR4WJFQSN+Q0REZLiMJPrbdLBixQq0bNkScrkccrkcCoUCO3fuVB7v3LkzJBKJyjZq1CiVPjIzM+Hv74/atWvDzs4OEydOxKNHj1TaHDhwAG3atIFMJoOrqyvi4uJ0/opYUSAiIqpiDRo0wLx58+Dm5gZBELB27Vr07t0bp0+fRvPmzQEAw4cPx6xZs5Tn1K5dW/nv0tJS+Pv7w8HBAUeOHMGtW7cQHByMWrVq4bPPPgMAZGRkwN/fH6NGjcL69euxb98+hIaGwtHREX5+flrHKhEEQdDTfVcbZq+Gix0CUaW7f2K52CEQVTrTSv5z1uztuXrrqyBpygudb2triwULFmDYsGHo3LkzWrdujcWLFz+17c6dOxEQEICbN2/C3t4eALBy5UpMnjwZd+7cgVQqxeTJk5GQkIBz584pzwsKCkJOTg527dqldVwceiAiIsMlkehtKyoqQl5enspWVFSkMYTS0lL88MMPePDgARQKhXL/+vXrUbduXbRo0QKRkZF4+PCh8lhycjK8vLyUSQIA+Pn5IS8vD+fPn1e28fHxUbmWn58fkpOTdfqKmCgQERHpQXR0NKysrFS26Ohote1TU1NhYWEBmUyGUaNGYcuWLfD09AQADBgwAN999x3279+PyMhIrFu3Dh988IHy3KysLJUkAYDyc1ZW1jPb5OXloaCgQOv74jMKRERkuPQ46yEyMhIREREq+2Qymdr27u7uSElJQW5uLn788UeEhITg4MGD8PT0xIgRI5TtvLy84OjoiK5du+LKlSto2rSp3mLWBhMFIiIyXHpcmVEmkz0zMXiSVCqFq6srAKBt27Y4ceIElixZgq+++qpC2/bt2wMA0tPT0bRpUzg4OOD48eMqbbKzswEADg4Oyv+3fN+/28jlcpiZmWkdJ4ceiIiIqoGysjK1zzSkpKQAABwdHQEACoUCqampuH37trJNYmIi5HK5cvhCoVBg3759Kv0kJiaqPAehDVYUiIjIcIm04FJkZCR69OiBRo0a4e+//0Z8fDwOHDiA3bt348qVK4iPj0fPnj1Rp04dnD17FuPGjUPHjh3RsmVLAICvry88PT0xaNAgzJ8/H1lZWZg6dSrCwsKUVY1Ro0Zh+fLlmDRpEoYOHYqkpCRs3LgRCQkJOsXKRIGIiAyXSC+Fun37NoKDg3Hr1i1YWVmhZcuW2L17N7p164br169j7969WLx4MR48eICGDRsiMDAQU6dOVZ5vbGyM7du3Y/To0VAoFDA3N0dISIjKugsuLi5ISEjAuHHjsGTJEjRo0ACrV6/WaQ0FgOsoEL20uI4CGYJKX0fB7wu99VWwe4Le+qpOWFEgIiLDxXc9aMREgYiIDJdIQw8vE6ZSREREpBYrCkREZLg49KAREwUiIjJcHHrQiKkUERERqcWKAhERGS4OPWjERIGIiAwXEwWN+A0RERGRWqwoEBGR4eLDjBoxUSAiIsPFoQeN+A0RERGRWqwoEBGR4eLQg0ZMFIiIyHBx6EEjfkNERESkFisKRERkuDj0oBETBSIiMlgSJgoaceiBiIiI1GJFgYiIDBYrCpoxUSAiIsPFPEEjDj0QERGRWqwoEBGRweLQg2ZMFIiIyGAxUdCMQw9ERESkFisKRERksFhR0IyJAhERGSwmCppx6IGIiIjUYkWBiIgMFwsKGjFRICIig8WhB8049EBERERqsaJAREQGixUFzZgoEBGRwWKioBmHHoiIiEgtVhSIiMhgsaKgGRMFIiIyXMwTNOLQAxEREanFigIRERksDj1oxkSBiIgMFhMFzTj0QERERGqxokBERAaLFQXNRE0UiouLsXXrViQnJyMrKwsA4ODggDfffBO9e/eGVCoVMzwiIqrpmCdoJNrQQ3p6Ojw8PBASEoLTp0+jrKwMZWVlOH36NIKDg9G8eXOkp6eLFR4RERFBxIrC6NGj4eXlhdOnT0Mul6scy8vLQ3BwMMLCwrB7926RIiQiopqOQw+aiZYo/Prrrzh+/HiFJAEA5HI5Zs+ejfbt24sQGRERGQomCpqJNvRgbW2Na9euqT1+7do1WFtbV1k8REREVJFoFYXQ0FAEBwdj2rRp6Nq1K+zt7QEA2dnZ2LdvH+bMmYMxY8aIFR4RERkAVhQ0Ey1RmDVrFszNzbFgwQKMHz9e+f9ZgiDAwcEBkydPxqRJk8QKj4iIDAATBc1EnR45efJkTJ48GRkZGSrTI11cXMQMi4iIiP6nWiy45OLiwuSAiIiqHgsKGlWLRIGIiEgMHHrQjO96ICIiIrWYKBARkcGSSCR623SxYsUKtGzZEnK5HHK5HAqFAjt37lQeLywsRFhYGOrUqQMLCwsEBgYiOztbpY/MzEz4+/ujdu3asLOzw8SJE/Ho0SOVNgcOHECbNm0gk8ng6uqKuLg4nb8jJgpERGSwxEoUGjRogHnz5uHkyZP47bff8Pbbb6N37944f/48AGDcuHH45ZdfsGnTJhw8eBA3b95Ev379lOeXlpbC398fxcXFOHLkCNauXYu4uDhMnz5d2SYjIwP+/v7o0qULUlJSMHbsWISGhuq84rFEEARBpzP0bNeuXbCwsECHDh0AALGxsVi1ahU8PT0RGxsLGxsbnfs0ezVc32ESVTv3TywXOwSiSmdayU/SNQzbpre+0hd1R1FRkco+mUwGmUym1fm2trZYsGAB3n33XdSrVw/x8fF49913AQAXL16Eh4cHkpOT8cYbb2Dnzp0ICAjAzZs3lesQrVy5EpMnT8adO3cglUoxefJkJCQk4Ny5c8prBAUFIScnB7t27dL6vkSvKEycOBF5eXkAgNTUVIwfPx49e/ZERkYGIiIiRI6OiIhqNIn+tujoaFhZWals0dHRGkMoLS3FDz/8gAcPHkChUODkyZMoKSmBj4+Psk2zZs3QqFEjJCcnAwCSk5Ph5eWlTBIAwM/PD3l5ecqqRHJyskof5W3K+9CW6LMeMjIy4OnpCQDYvHkzAgIC8Nlnn+HUqVPo2bOnyNEREVFNps9ZD5GRkRX+wH1WNSE1NRUKhQKFhYWwsLDAli1b4OnpiZSUFEil0gqvMbC3t1euOZSVlaWSJJQfLz/2rDZ5eXkoKCiAmZmZVvcleqIglUrx8OFDAMDevXsRHBwM4HEJprzSQEREVN3pMswAAO7u7khJSUFubi5+/PFHhISE4ODBg5UY4fMRPVHo0KEDIiIi4O3tjePHj2PDhg0AgEuXLqFBgwYiR0dERDWZmOsoSKVSuLq6AgDatm2LEydOYMmSJejfvz+Ki4uRk5OjUlXIzs6Gg4MDgMerGB8/flylv/JZEf9u8+RMiezsbMjlcq2rCUA1SBSWL1+ODz/8ED/++CNWrFiB+vXrAwB27tyJ7t27ixyd4Rj+nw4Y/u5bcHayBQD8fjULn329E3t+vYBGjrZI2zHrqecNnPgNftp7GgCwcNK7eKNVEzR3dcTFjGy8ETSvQnsfhQemjeoJj6aOKCwuwa+nrmDywp+Qeete5d0ckRorYpdh5ZeqD4U2dnHBtu3/POh1JuU0li2JQWrqWRgbGcG9mQdWfP0NTE1NlW0OHTyAr1bE4vKlNEhlMrRr9xoWL/uyyu6Dnl91WnCprKwMRUVFaNu2LWrVqoV9+/YhMDAQAJCWlobMzEwoFAoAgEKhwNy5c3H79m3Y2dkBABITEyGXy5XD+QqFAjt27FC5RmJiorIPbYmeKDRq1Ajbt2+vsD8mJkaEaAzXjewcTFu2DemZdyCBBB/0ao9NMSPwRtA8pF3LRmOfSJX2QwO9MS7YB7t/Pa+y//+2HcVrXs5o4Va/wjWcnepgU8wILP0uCYOnrIWVhSnmTwjEDwuH480Bn1fq/RGp09TVDV+vXqP8bGxirPz3mZTT+HBkKIaGjsQnU6bBxNgYaWkXYWT0z3Pge/fsxswZ0zBm7Di83v4NlD4qRXr6pSq9B3r5REZGokePHmjUqBH+/vtvxMfH48CBA9i9ezesrKwwbNgwREREwNbWFnK5HGPGjIFCocAbb7wBAPD19YWnpycGDRqE+fPnIysrC1OnTkVYWJhy+GPUqFFYvnw5Jk2ahKFDhyIpKQkbN25EQkKCTrGKniicOnUKtWrVgpeXFwBg27ZtWLNmDTw9PREVFQWpVCpyhIZhx6FzKp+jYn/B8P90wOstXfD71Sxk3/1b5fg7XVphc+IpPCgoVu4bP/9HAEBdm55PTRTaeDaEsZERomK3o3xW7uL/24dNMSNgYmKER4/K9H1bRBqZGBujbr16Tz224PNovD9wEIYNH6Hc19ilifLfjx49wufz5mLchInoF/gf5f6m/ysnU/UnVkXh9u3bCA4Oxq1bt2BlZYWWLVti9+7d6NatG4DHfywbGRkhMDAQRUVF8PPzw5df/lOlMjY2xvbt2zF69GgoFAqYm5sjJCQEs2b9U/11cXFBQkICxo0bhyVLlqBBgwZYvXo1/Pz8dIpV9ERh5MiR+OSTT+Dl5YWrV68iKCgIffv2xaZNm/Dw4UMsXrxY7BANjpGRBIHd2sDcTIpjZzMqHH/VoyFaN2uIcfM26tTvqQvXUSaUIbj3G1j381FY1JZhgP/rSDqWxiSBRPNH5h/w6dwBUpkMrVq1xkdjx8PRyQl3795F6tkz6BnQC8EDg3D9eiZcXJog/KOxaNO2HQDg9wsXcDs7G0ZGRngvsA/u/vUX3Js1w7gJk+Dm9orId0ZaEWnk4ZtvvnnmcVNTU8TGxiI2NlZtG2dn5wpDC0/q3LkzTp8+/VwxlhN9HYVLly6hdevWAIBNmzahY8eOiI+PR1xcHDZv3qzx/KKiIuTl5alsQllpJUddMzV3dcKdXxci99hiLJ3SH/3Hr8LFq1kV2oX0UeD3q7dw9EzFJOJZ/rh5FwEfxmJmeC/kHluM7P9+gfr21vhg0rf6ugUinXi1bInZc6Px5VerMWVaFG7cuIEhwQPx4EE+bvx5HQCwMnY5+r37H3z51Wp4eHhixLDB+OOPawCAP//VZsTI0Vj25UrI5VYIHTwIuTk5It0VkX6JnigIgoCyssd/Te7du1e5dkLDhg3x119/aTz/aQtcPMo+Wakx11SXrmWjfVA0OgZ/gVWbDmPVrEFo1sRBpY2prBb692iHtVt1W7ADAOzrWOLLaQOw/pdj6PDBAvgMi0FxSSnivximr1sg0kmHtzrB168HXnFvBu8Ob2H5iq/x99952L1rp/K/S+++1x99+gbCw8MTEz/5FI1dXLD1p8d/xAj/axM6YhR8fP3g2bwFZs2NhkQiwZ492q98R+IRawnnl4noiUK7du0wZ84crFu3DgcPHoS/vz+AxwsxPblQxNNERkYiNzdXZTOxb1vZYddIJY9KcfX6Xzj9+3VMX/YzUi/dQNj7nVXa9PVpjdqmUqzffvzpnTzDyP4dkZdfgClLtuFM2p/49dQVDJ2yFm+3b4bXvRrr5yaIXoBcLoezc2Ncz8xUPrfQpGlTlTYuTZoi69ZNAHhqG6lUivoNGiLr1q0qippeBBMFzURPFBYvXoxTp04hPDwcU6ZMUc4p/fHHH/Hmm29qPF8mkynfvlW+SYyMNZ5HmhlJJJBJVR9jGdznTSQcTMVf9/N17q+2qRRlZaqvFin9319kRkY1939k9PJ4+OABrl+/jrr16qF+/QaoZ2eHaxmqQ2x/XLsGR6fHD+t6Nm8BqVSKa9f+aVNSUoKbN2/A0dGpSmMnqiyiP8zYsmVLpKamVti/YMECGBvzF35VmTXmHez+9Tyu37oPS3NT9O/RDh3buaHXh/88ZdukYV10aNMUfcaseGofTRrWhYWZDPZ15TCT1ULLVx7/x/T3q1koeVSKnf89jzEDuyByRHds3HUSlrVlmBn+Dv64eRcpF/+skvsk+reFCz5Hp85d4OjkhDu3b2NF7DIYGxuhR88ASCQSDB4yDCtil8HdvRncm3ng521bcC3jKhbGLAUAWFhY4D/vBWFF7DI4ODjCyckJcWseP6Tm68d1YF4GNbgQoDeiJwrq/HsxE6p89Wwt8M3sYDjUlSM3vxDnLt9Arw+/RNKxi8o2Ib0VuJGdg73JF5/ax4rpA9GxnZvy87ENj9decO85HZm37uHgiUsY/OlajAvxQURINzwsLMaxsxl4J+xLFBaVVO4NEj1FdnYWPpkYgZycHNjY2uLVNm2xLn4jbG0fLzz2QfBgFBUVY8H8aOTm5sLdvRlWrvoWDRs1UvYxbsIkGJuYYErkJBQVFsKrZSus+nYt5FZWYt0W6aAmDxnoi+ivmS4tLUVMTAw2btyIzMxMFBcXqxy/d0/3Ffv4mmkyBHzNNBmCyn7NtNtE/T10enlBzawiif6MwsyZM7Fo0SL0798fubm5iIiIQL9+/WBkZISoqCixwyMiohpMItHfVlOJniisX78eq1atwvjx42FiYoL3338fq1evxvTp03H06FGxwyMiohqMsx40Ez1RyMrKUi7fbGFhgdzcXABAQECAzutRExERkX6Jnig0aNAAt/4337hp06bYs2cPAODEiRM6vdebiIhIVxx60Ez0RKFv377Yt28fAGDMmDGYNm0a3NzcEBwcjKFDh4ocHRER1WRGRhK9bTWV6NMj582bp/x3//790ahRIyQnJ8PNzQ29evUSMTIiIiISPVF4kkKhgEKhEDsMIiIyADV5yEBfREkUfv75Z63bvvPOO5UYCRERET2LKIlCnz59tGonkUhQWspXRhMRUeWoydMa9UWURKH89a1ERERiYp6gmeizHoiIiKj6Ei1RSEpKgqenJ/Ly8iocy83NRfPmzXHo0CERIiMiIkPBlRk1Ey1RWLx4MYYPHw65XF7hmJWVFUaOHImYmBgRIiMiIkPBREEz0RKFM2fOoHt39W/a8vX1xcmTJ6swIiIiInqSaOsoZGdno1atWmqPm5iY4M6dO1UYERERGZoaXAjQG9EqCvXr18e5c+fUHj979iwcHR2rMCIiIjI0HHrQTLREoWfPnpg2bRoKCwsrHCsoKMCMGTMQEBAgQmRERERUTrShh6lTp+Knn37CK6+8gvDwcLi7uwMALl68iNjYWJSWlmLKlClihUdERAagBhcC9Ea0RMHe3h5HjhzB6NGjERkZCUEQADwuA/n5+SE2Nhb29vZihUdERAagJg8Z6IuoL4VydnbGjh07cP/+faSnp0MQBLi5ucHGxkbMsIiIiOh/qsXbI21sbPDaa6+JHQYRERkYFhQ0qxaJAhERkRg49KAZ3/VAREREarGiQEREBosFBc2YKBARkcHi0INmHHogIiIitVhRICIig8WCgmZMFIiIyGBx6EEzDj0QERGRWqwoEBGRwWJBQTMmCkREZLA49KAZhx6IiIhILVYUiIjIYLGgoBkTBSIiMlgcetCMQw9ERESkFisKRERksFhR0IyJAhERGSzmCZpx6IGIiIjUYkWBiIgMFoceNGOiQEREBot5gmYceiAiIiK1WFEgIiKDxaEHzZgoEBGRwWKeoBmHHoiIiKpYdHQ0XnvtNVhaWsLOzg59+vRBWlqaSpvOnTtDIpGobKNGjVJpk5mZCX9/f9SuXRt2dnaYOHEiHj16pNLmwIEDaNOmDWQyGVxdXREXF6dTrEwUiIjIYBlJJHrbdHHw4EGEhYXh6NGjSExMRElJCXx9ffHgwQOVdsOHD8etW7eU2/z585XHSktL4e/vj+LiYhw5cgRr165FXFwcpk+frmyTkZEBf39/dOnSBSkpKRg7dixCQ0Oxe/durWPl0AMRERksfQ49FBUVoaioSGWfTCaDTCar0HbXrl0qn+Pi4mBnZ4eTJ0+iY8eOyv21a9eGg4PDU6+3Z88eXLhwAXv37oW9vT1at26N2bNnY/LkyYiKioJUKsXKlSvh4uKChQsXAgA8PDxw+PBhxMTEwM/PT6v7YkWBiIhID6Kjo2FlZaWyRUdHa3Vubm4uAMDW1lZl//r161G3bl20aNECkZGRePjwofJYcnIyvLy8YG9vr9zn5+eHvLw8nD9/XtnGx8dHpU8/Pz8kJydrfV+sKBARkcHS56yHyMhIREREqOx7WjXhSWVlZRg7diy8vb3RokUL5f4BAwbA2dkZTk5OOHv2LCZPnoy0tDT89NNPAICsrCyVJAGA8nNWVtYz2+Tl5aGgoABmZmYa42OiQEREBstIj0MP6oYZNAkLC8O5c+dw+PBhlf0jRoxQ/tvLywuOjo7o2rUrrly5gqZNm75wvNri0AMREZFIwsPDsX37duzfvx8NGjR4Ztv27dsDANLT0wEADg4OyM7OVmlT/rn8uQZ1beRyuVbVBICJAhERGbAnpx++yKYLQRAQHh6OLVu2ICkpCS4uLhrPSUlJAQA4OjoCABQKBVJTU3H79m1lm8TERMjlcnh6eirb7Nu3T6WfxMREKBQKrWNlokBERAZLItHfpouwsDB89913iI+Ph6WlJbKyspCVlYWCggIAwJUrVzB79mycPHkS165dw88//4zg4GB07NgRLVu2BAD4+vrC09MTgwYNwpkzZ7B7925MnToVYWFhyiGQUaNG4erVq5g0aRIuXryIL7/8Ehs3bsS4ceO0jpWJAhERURVbsWIFcnNz0blzZzg6Oiq3DRs2AACkUin27t0LX19fNGvWDOPHj0dgYCB++eUXZR/GxsbYvn07jI2NoVAo8MEHHyA4OBizZs1StnFxcUFCQgISExPRqlUrLFy4EKtXr9Z6aiQASARBEHS5ubVr16Ju3brw9/cHAEyaNAlff/01PD098f3338PZ2VmX7iqF2avhYodAVOnun1gudghElc60kh+5D/jqhN762j7yNb31VZ3oXFH47LPPlA9AJCcnIzY2FvPnz0fdunV1KmUQERGJzUiiv62m0jlXu379OlxdXQEAW7duRWBgIEaMGAFvb2907txZ3/ERERGRiHSuKFhYWODu3bsAHi8f2a1bNwCAqamp8iEMIiKil4FYsx5eJjpXFLp164bQ0FC8+uqruHTpEnr27AkAOH/+PBo3bqzv+IiIiCpNDf79rjc6VxRiY2OhUChw584dbN68GXXq1AEAnDx5Eu+//77eAyQiIiLx6FxRsLa2xvLlFZ+2njlzpl4CIiIiqiq6vh7aEGmVKJw9e1brDssXgiAiIqrumCdoplWi0Lp1a0gkEqhbcqH8mEQiQWlpqV4DJCIiIvFolShkZGRUdhxERERVribPVtAXrRKF6rDaIhERkb4xT9Dsud71sG7dOnh7e8PJyQl//PEHAGDx4sXYtm2bXoMjIiIicemcKKxYsQIRERHo2bMncnJylM8kWFtbY/HixfqOj4iIqNIYSSR622oqnROFZcuWYdWqVZgyZQqMjY2V+9u1a4fU1FS9BkdERFSZJHrcaiqdE4WMjAy8+uqrFfbLZDI8ePBAL0ERERFR9aBzouDi4oKUlJQK+3ft2gUPDw99xERERFQl+K4HzXRemTEiIgJhYWEoLCyEIAg4fvw4vv/+e0RHR2P16tWVESMREVGlqMmvh9YXnROF0NBQmJmZYerUqXj48CEGDBgAJycnLFmyBEFBQZURIxEREYlE50QBAAYOHIiBAwfi4cOHyM/Ph52dnb7jIiIiqnQ1echAX54rUQCA27dvIy0tDcDjL7pevXp6C4qIiKgqME/QTOeHGf/++28MGjQITk5O6NSpEzp16gQnJyd88MEHyM3NrYwYiYiISCQ6JwqhoaE4duwYEhISkJOTg5ycHGzfvh2//fYbRo4cWRkxEhERVQrOetBM56GH7du3Y/fu3ejQoYNyn5+fH1atWoXu3bvrNTgiIqLKxFkPmulcUahTpw6srKwq7LeysoKNjY1egiIiIqLqQedEYerUqYiIiEBWVpZyX1ZWFiZOnIhp06bpNTgiIqLKxKEHzbQaenj11VdVvoTLly+jUaNGaNSoEQAgMzMTMpkMd+7c4XMKRET00qi5v971R6tEoU+fPpUcBhEREVVHWiUKM2bMqOw4iIiIqlxNfj20vjz3gktEREQvO+YJmumcKJSWliImJgYbN25EZmYmiouLVY7fu3dPb8ERERGRuHSe9TBz5kwsWrQI/fv3R25uLiIiItCvXz8YGRkhKiqqEkIkIiKqHJz1oJnOicL69euxatUqjB8/HiYmJnj//fexevVqTJ8+HUePHq2MGImIiCqFRKK/rabSOVHIysqCl5cXAMDCwkL5foeAgAAkJCToNzoiIiISlc6JQoMGDXDr1i0AQNOmTbFnzx4AwIkTJyCTyfQbHRERUSUykkj0ttVUOicKffv2xb59+wAAY8aMwbRp0+Dm5obg4GAMHTpU7wESERFVFg49aKbzrId58+Yp/92/f384OzvjyJEjcHNzQ69evfQaHBEREYlL54rCk9544w1ERESgffv2+Oyzz/QRExERUZXgrAfNJIIgCPro6MyZM2jTpg1KS0v10d0LuZP/SOwQiCqd64jvxQ6BqNLlxg+q1P7HbPldb30t6+uht76qkxeuKBAREVHNxSWciYjIYNXkIQN9YaJAREQGy4h5gkZaJwoRERHPPH7nzp0XDoaIiIiqF60ThdOnT2ts07FjxxcKhoiIqCqxoqCZ1onC/v37KzMOIiKiKsdnFDTjrAciIiJSiw8zEhGRweLQg2ZMFIiIyGBx5EEzDj0QERGRWqwoEBGRwarJr4fWl+eqKPz3v//FBx98AIVCgRs3bgAA1q1bh8OHD+s1OCIiospkpMetptL53jZv3gw/Pz+YmZnh9OnTKCoqAgDk5uby7ZFEREQ1jM6Jwpw5c7By5UqsWrUKtWrVUu739vbGqVOn9BocERFRZZJI9LfpIjo6Gq+99hosLS1hZ2eHPn36IC0tTaVNYWEhwsLCUKdOHVhYWCAwMBDZ2dkqbTIzM+Hv74/atWvDzs4OEydOxKNHqm9QPnDgANq0aQOZTAZXV1fExcXpFKvOiUJaWtpTV2C0srJCTk6Ort0RERGJxkgi0dumi4MHDyIsLAxHjx5FYmIiSkpK4OvriwcPHijbjBs3Dr/88gs2bdqEgwcP4ubNm+jXr5/yeGlpKfz9/VFcXIwjR45g7dq1iIuLw/Tp05VtMjIy4O/vjy5duiAlJQVjx45FaGgodu/erXWsOj/M6ODggPT0dDRu3Fhl/+HDh9GkSRNduyMiIjI4u3btUvkcFxcHOzs7nDx5Eh07dkRubi6++eYbxMfH4+233wYArFmzBh4eHjh69CjeeOMN7NmzBxcuXMDevXthb2+P1q1bY/bs2Zg8eTKioqIglUqxcuVKuLi4YOHChQAADw8PHD58GDExMfDz89MqVp0rCsOHD8fHH3+MY8eOQSKR4ObNm1i/fj0mTJiA0aNH69odERGRaPQ59FBUVIS8vDyVrfw5Pk1yc3MBALa2tgCAkydPoqSkBD4+Pso2zZo1Q6NGjZCcnAwASE5OhpeXF+zt7ZVt/Pz8kJeXh/Pnzyvb/LuP8jblfWhD50Thk08+wYABA9C1a1fk5+ejY8eOCA0NxciRIzFmzBhduyMiIhKNkUR/W3R0NKysrFS26OhojTGUlZVh7Nix8Pb2RosWLQAAWVlZkEqlsLa2Vmlrb2+PrKwsZZt/Jwnlx8uPPatNXl4eCgoKtPqOdB56kEgkmDJlCiZOnIj09HTk5+fD09MTFhYWunZFRERUY0RGRiIiIkJln0wm03heWFgYzp07V22XGHjuBZekUik8PT31GQsREVGV0ueCSzKZTKvE4N/Cw8Oxfft2HDp0CA0aNFDud3BwQHFxMXJyclSqCtnZ2XBwcFC2OX78uEp/5bMi/t3myZkS2dnZkMvlMDMz0ypGnROFLl26PPO1nElJSbp2SUREJAqxFmYUBAFjxozBli1bcODAAbi4uKgcb9u2LWrVqoV9+/YhMDAQwONZh5mZmVAoFAAAhUKBuXPn4vbt27CzswMAJCYmQi6XK/+QVygU2LFjh0rfiYmJyj60oXOi0Lp1a5XPJSUlSElJwblz5xASEqJrd0RERAYnLCwM8fHx2LZtGywtLZXPFFhZWcHMzAxWVlYYNmwYIiIiYGtrC7lcjjFjxkChUOCNN94AAPj6+sLT0xODBg3C/PnzkZWVhalTpyIsLExZ2Rg1ahSWL1+OSZMmYejQoUhKSsLGjRuRkJCgdaw6JwoxMTFP3R8VFYX8/HxduyMiIhKNWK+ZXrFiBQCgc+fOKvvXrFmDwYMHA3j8+9bIyAiBgYEoKiqCn58fvvzyS2VbY2NjbN++HaNHj4ZCoYC5uTlCQkIwa9YsZRsXFxckJCRg3LhxWLJkCRo0aIDVq1drPTUSACSCIAjPf6v/SE9Px+uvv4579+7po7sXcif/keZGRC851xHfix0CUaXLjR9Uqf1/tu+K3vr6tGtTvfVVnejtPRbJyckwNTXVV3dERERUDeg89PDv5SOBxw9k3Lp1C7/99humTZumt8CIiIgqm1hDDy8TnRMFKysrlc9GRkZwd3fHrFmz4Ovrq7fAiIiIKhsTBc10ShRKS0sxZMgQeHl5wcbGprJiIiIiompCp2cUjI2N4evry7dEEhFRjSCRSPS21VQ6P8zYokULXL16tTJiISIiqlL6fNdDTaVzojBnzhxMmDAB27dvx61btyq8KYuIiIhqDq2fUZg1axbGjx+Pnj17AgDeeecdlVKLIAiQSCQoLS3Vf5RERESVoAaPGOiN1onCzJkzMWrUKOzfv78y4yEiIqoy+nwpVE2ldaJQvoBjp06dKi0YIiIiql50mh5Zk5/qJCIiw1OTH0LUF50ShVdeeUVjslAd3vVARESkDf79q5lOicLMmTMrrMxIRERENZdOiUJQUBDs7OwqKxYiIqIqZQSWFDTROlHg8wlERFTT8FebZlovuFQ+64GIiIgMh9YVhbKyssqMg4iIqMpx1oNmOr9mmoiIqKbggkua6fyuByIiIjIcrCgQEZHBYkFBMyYKRERksDj0oBmHHoiIiEgtVhSIiMhgsaCgGRMFIiIyWCyra8bviIiIiNRiRYGIiAwWX0+gGRMFIiIyWEwTNOPQAxEREanFigIRERksrqOgGRMFIiIyWEwTNOPQAxEREanFigIRERksjjxoxkSBiIgMFqdHasahByIiIlKLFQUiIjJY/GtZMyYKRERksDj0oBmTKSIiIlKLFQUiIjJYrCdoxkSBiIgMFoceNOPQAxEREanFigIRERks/rWsGRMFIiIyWBx60IzJFBEREanFigIRERks1hM0Y6JAREQGiyMPmnHogYiIiNRiRYGIiAyWEQcfNGKiQEREBotDD5px6IGIiIjUqraJQnZ2NmbNmiV2GEREVINJ9Ph/NVW1TRSysrIwc+ZMscMgIqIaTCLR36aLQ4cOoVevXnBycoJEIsHWrVtVjg8ePBgSiURl6969u0qbe/fuYeDAgZDL5bC2tsawYcOQn5+v0ubs2bN46623YGpqioYNG2L+/Pk6f0eiPaNw9uzZZx5PS0urokiIiIiq1oMHD9CqVSsMHToU/fr1e2qb7t27Y82aNcrPMplM5fjAgQNx69YtJCYmoqSkBEOGDMGIESMQHx8PAMjLy4Ovry98fHywcuVKpKamYujQobC2tsaIESO0jlW0RKF169aQSCQQBKHCsfL9XFqTiIgqk1izHnr06IEePXo8s41MJoODg8NTj/3+++/YtWsXTpw4gXbt2gEAli1bhp49e+KLL76Ak5MT1q9fj+LiYnz77beQSqVo3rw5UlJSsGjRIp0SBdGGHmxtbbFq1SpkZGRU2K5evYrt27eLFRoRERkIfQ49FBUVIS8vT2UrKip67tgOHDgAOzs7uLu7Y/To0bh7967yWHJyMqytrZVJAgD4+PjAyMgIx44dU7bp2LEjpFKpso2fnx/S0tJw//59reMQLVFo27Ytbt68CWdn56du9evXf2q1gYiIqDqKjo6GlZWVyhYdHf1cfXXv3h3/93//h3379uHzzz/HwYMH0aNHD5SWlgJ4/ByfnZ2dyjkmJiawtbVFVlaWso29vb1Km/LP5W20IdrQw6hRo/DgwQO1xxs1aqQyNkNERKRv+hzhjoyMREREhMq+J58r0FZQUJDy315eXmjZsiWaNm2KAwcOoGvXri8Up65ESxT69u37zOM2NjYICQmpomiIiMgQ6XNao0wme+7EQJMmTZqgbt26SE9PR9euXeHg4IDbt2+rtHn06BHu3bunfK7BwcEB2dnZKm3KP6t79uFpqu30SCIiInrszz//xN27d+Ho6AgAUCgUyMnJwcmTJ5VtkpKSUFZWhvbt2yvbHDp0CCUlJco2iYmJcHd3h42NjdbXZqJAREQGy0iiv00X+fn5SElJQUpKCgAgIyMDKSkpyMzMRH5+PiZOnIijR4/i2rVr2LdvH3r37g1XV1f4+fkBADw8PNC9e3cMHz4cx48fx6+//orw8HAEBQXByckJADBgwABIpVIMGzYM58+fx4YNG7BkyZIKwyOa8F0PRERksMRaUfG3335Dly5dlJ/Lf3mHhIRgxYoVOHv2LNauXYucnBw4OTnB19cXs2fPVhnaWL9+PcLDw9G1a1cYGRkhMDAQS5cuVR63srLCnj17EBYWhrZt26Ju3bqYPn26TlMjAUAi1MCpBXfyH4kdAlGlcx3xvdghEFW63PhBldp/0sW7mhtp6e1mdfTWV3XCigIRERksruunmejPKOzatQuHDx9Wfo6NjUXr1q0xYMAAnRaEICIi0hVfCqWZ6InCxIkTkZeXBwBITU3F+PHj0bNnT2RkZOj8wAURERHpl+hDDxkZGfD09AQAbN68GQEBAfjss89w6tQp9OzZU+ToiIioJtN1toIhEr2iIJVK8fDhQwDA3r174evrC+DxuyDKKw1ERESVgUMPmoleUejQoQMiIiLg7e2N48ePY8OGDQCAS5cuoUGDBiJHZ7jeDeiGrFs3K+zv+58gjP9kGgDg3NkUfB27BBfOpcLI2AhurzTDouVfQ2Zqqmx/5L8HsWbVClxJvwSpVIZX27RD9KJlVXYfRP82zOcVDPV5BY3qmgMALt7Ixec/ncXeMzdhYy5F5Lut8LaXIxrUNcdfeUVI+O065m5KQV7BPwvWfB78Gt5wrwePBtZIu5GLtz5NULnGJ4EtERnYqsK1HxQ+gtNQzlShl4/oicLy5cvx4Ycf4scff8SKFStQv359AMDOnTvRvXt3kaMzXKvWbUDZ/14+AgBXr6Rj3Ieh6OLzeLGPc2dTMD58JD4YEoqxk6bAxNgYly+lQWL0T5HqwL49+HzODIwMG4s2r7VHaekjXE1Pr/J7ISp3495DRP1wCley/oYEwICOTfH9+M54KzIBEokEjjZmmBp/Cml/5qBhXQvEDGsPRxszBC85pNLPugPpaOdaF80bVlzdbtn2C/h27yWVfT9P6YZTV/Q3DY/0h7MeNOM6CqSVJV9E48h/D+KHrTshkUgwIuR9vNZegeEffvTU9o8ePcJ/evli2MgwBPQJrOJoDQPXUdCPa1+/h2nxp7DuQMUktk/7Rvj6ww5wHPI9SstU/1P5SWBL+LdtWKGi8KQWjWzw67wAdJ+5G8lpt5/Zliqq7HUUfr2sv9l13m7aL4v8MhH9GYVTp04hNTVV+Xnbtm3o06cPPv30UxQXF4sYGZUrKSnGnh3b4d+7HyQSCe7fu4sL587CxrYORg0ZiF7dOiJ8eAjOnP5nzfFLFy/gzu1sSIyMMGRAIHr7dsL4MSNxNf2yiHdC9A8jiQSBisaoLTPB8ct3ntpGbibF3wUlFZIEXQR3ccXlm7lMEuilJXqiMHLkSFy69LhMd/XqVQQFBaF27drYtGkTJk2apPH8oqIi5OXlqWxFRUWVHbZBObQ/Cfn5f6Nnrz4AgBs3/gQAfPt1LHr1fRcLl32FV5p5YOzoYbie+QcA4GZ5m69iETJsJD5f8iUsLeUYM2Iw8nJzxLgNIgCAZ0Nr3Pg2CHf+bwAWDW2PgTEHkHYjt0I7W0sZJvb1QlzS8ye3slpGeM/b5anVCqoejCQSvW01leiJwqVLl9C6dWsAwKZNm9CxY0fEx8cjLi4Omzdv1nh+dHQ0rKysVLYlCz+v5KgNS8K2zWj/ZgfUrWcHABDKygAAvfu9B/93+uKVZh74aPwnaOTsgoRtPwEAyv7XJnjYCHTu6otmHs3xadRcSCQSJO3dI86NEAG4fDMPb0UmoOv0nfh27yWsHOUN9/pWKm0szWph08S3kXYjF9Gbzzz3tQLaNYKFaS3EH7r6omFTJZHocaupRH+YURAE5S+VvXv3IiAgAADQsGFD/PXXXxrPj4yMrLAwU16Jsf4DNVBZt27it+NHMXfBEuW+OnXrAQAaN2mq0tbZpQmys24BAOqWt3H5p41UKoVj/QbKNkRiKCktw9XsvwEAKRn30KZpHYzu3gxjvzkGALAwNcHmyW8jv7AEA2MO4FHp8w87hHRxxa7Tf+JOXqFeYicSg+gVhXbt2mHOnDlYt24dDh48CH9/fwCPF2Kyt7fXeL5MJoNcLlfZ/v12LXoxCT9vgY2NLRQdOir3OTrVR916dsi8lqHS9nrmNTg4Pn69qbtHc0ilUlz/45ry+KOSEmTdugmH/71Pnag6MJJIIDV5/MeFpVktbIn0QfGjMgR9sR9FJWXP3a9zPQu85enAYYfqjiUFjUSvKCxevBgDBw7E1q1bMWXKFLi6ugIAfvzxR7z55psiR2fYysrKsOPnLege0BsmJv/8qEgkEgwIHoJvVsbC9RV3uLk3w85ftuGPaxmY83kMAMDcwgK9A9/DN1/Fws7eAQ6OToj/vzUAoJxiSVTVZvR/FYlnbuDPvx7AwqwW/vOmCzp42KPfvH2Pk4RPusJMZoIRsYdhaVYLlma1AAB/5RWh7H8TxJrYW8Lc1AT2VmYwkxrDy/nxk+4X/8xFSek/icUHnZsiK6cAiSkV1yOh6qMmL5SkL6InCi1btlSZ9VBuwYIFMDbmEIKYfjuWjOysW/Dv3a/CsfcGBKOoqAjLFs1HXm4uXF9xR0zsKtRv2EjZJuzjCTA2NsHs6ZEoKiqEZ4uWWLLyW8jlVhX6I6oK9eSmWDnaGw7WZsh7WILz1++j37x92H/uFjp42OM1t8dDZimL+6qc5/XRT8j86wEAYOnwN/CWp4Py2OHogAptJJLHazTEH7qiTDCIXlZcR4HoJcV1FMgQVPY6CsevVpzx8rxeb1Iz/wgSvaJQWlqKmJgYbNy4EZmZmRXWTrh3755IkRERUU3HgQfNRH+YcebMmVi0aBH69++P3NxcREREoF+/fjAyMkJUVJTY4RERERk00ROF9evXY9WqVRg/fjxMTEzw/vvvY/Xq1Zg+fTqOHj0qdnhERFSTcdaDRqInCllZWfDy8gIAWFhYIDf38XhRQEAAEhKevYY6ERHRi+BrpjUTPVFo0KABbt16vABP06ZNsWfP41X7Tpw4wfUQiIiIRCZ6otC3b1/s27cPADBmzBhMmzYNbm5uCA4OxtChQ0WOjoiIajKJRH9bTSX6rId58+Yp/92/f380atQIycnJcHNzQ69evUSMjIiIiERPFJ6kUCigUCjEDoOIiAxADS4E6I0oicLPP/+sddt33nmnEiMhIiKDxkxBI1EShT59+mjVTiKRoLS0tHKDISIiIrVESRTKXytNREQkppo8rVFfqt0zCkRERFWlJs9W0BfRpkcmJSXB09MTeXl5FY7l5uaiefPmOHTokAiRERERUTnREoXFixdj+PDhkMvlFY5ZWVlh5MiRiImJESEyIiIyFFzBWTPREoUzZ86ge/fuao/7+vri5MmTVRgREREZHGYKGomWKGRnZ6NWrVpqj5uYmODOnTtVGBERERE9SbREoX79+jh37pza42fPnoWjo2MVRkRERIaGL4XSTLREoWfPnpg2bRoKCwsrHCsoKMCMGTMQEBAgQmRERGQo+K4HzSSCIAhiXDg7Oxtt2rSBsbExwsPD4e7uDgC4ePEiYmNjUVpailOnTsHe3l7nvu/kP9J3uETVjuuI78UOgajS5cYPqtT+U//M11tfXg0s9NZXdSLaOgr29vY4cuQIRo8ejcjISJTnKxKJBH5+foiNjX2uJIGIiEhbNbgQoDeiLrjk7OyMHTt24P79+0hPT4cgCHBzc4ONjY2YYRERkaFgpqBRtViZ0cbGBq+99prYYRAREdETqkWiQEREJIaaPFtBX5goEBGRwarJsxX0RbTpkURERFT9saJAREQGiwUFzZgoEBGR4WKmoBGHHoiIiEgtVhSIiMhgcdaDZkwUiIjIYHHWg2YceiAiIiK1WFEgIiKDxYKCZkwUiIjIcDFT0IhDD0RERKQWEwUiIjJYEj3+ny4OHTqEXr16wcnJCRKJBFu3blU5LggCpk+fDkdHR5iZmcHHxweXL19WaXPv3j0MHDgQcrkc1tbWGDZsGPLz81XanD17Fm+99RZMTU3RsGFDzJ8/X+fviIkCEREZLIlEf5suHjx4gFatWiE2Nvapx+fPn4+lS5di5cqVOHbsGMzNzeHn54fCwkJlm4EDB+L8+fNITEzE9u3bcejQIYwYMUJ5PC8vD76+vnB2dsbJkyexYMECREVF4euvv9btOxIEQdDt9qq/O/mPxA6BqNK5jvhe7BCIKl1u/KBK7T/9doHe+nK1M3uu8yQSCbZs2YI+ffoAeFxNcHJywvjx4zFhwgQAQG5uLuzt7REXF4egoCD8/vvv8PT0xIkTJ9CuXTsAwK5du9CzZ0/8+eefcHJywooVKzBlyhRkZWVBKpUCAD755BNs3boVFy9e1Do+VhSIiMhgSfS4FRUVIS8vT2UrKirSOaaMjAxkZWXBx8dHuc/Kygrt27dHcnIyACA5ORnW1tbKJAEAfHx8YGRkhGPHjinbdOzYUZkkAICfnx/S0tJw//59reNhokBERIZLj5lCdHQ0rKysVLbo6GidQ8rKygIA2Nvbq+y3t7dXHsvKyoKdnZ3KcRMTE9ja2qq0eVof/76GNjg9koiISA8iIyMRERGhsk8mk4kUjf4wUSAiIoOlz3c9yGQyvSQGDg4OAIDs7Gw4Ojoq92dnZ6N169bKNrdv31Y579GjR7h3757yfAcHB2RnZ6u0Kf9c3kYbHHogIiKDJdash2dxcXGBg4MD9u3bp9yXl5eHY8eOQaFQAAAUCgVycnJw8uRJZZukpCSUlZWhffv2yjaHDh1CSUmJsk1iYiLc3d1hY2OjdTxMFIiIiKpYfn4+UlJSkJKSAuDxA4wpKSnIzMyERCLB2LFjMWfOHPz8889ITU1FcHAwnJyclDMjPDw80L17dwwfPhzHjx/Hr7/+ivDwcAQFBcHJyQkAMGDAAEilUgwbNgznz5/Hhg0bsGTJkgrDI5pw6IGIiAyWWCs4//bbb+jSpYvyc/kv75CQEMTFxWHSpEl48OABRowYgZycHHTo0AG7du2Cqamp8pz169cjPDwcXbt2hZGREQIDA7F06VLlcSsrK+zZswdhYWFo27Yt6tati+nTp6ustaANrqNA9JLiOgpkCCp7HYVrdws1N9JS4zqmmhu9hDj0QERERGpx6IGIiAyWPmc91FRMFIiIyGDpc7ZCTcWhByIiIlKLFQUiIjJYLChoxkSBiIgMFoceNOPQAxEREanFigIRERkwlhQ0YaJAREQGi0MPmnHogYiIiNRiRYGIiAwWCwqaMVEgIiKDxaEHzTj0QERERGqxokBERAaL73rQjIkCEREZLuYJGnHogYiIiNRiRYGIiAwWCwqaMVEgIiKDxVkPmnHogYiIiNRiRYGIiAwWZz1oxkSBiIgMF/MEjTj0QERERGqxokBERAaLBQXNmCgQEZHB4qwHzTj0QERERGqxokBERAaLsx40Y6JAREQGi0MPmnHogYiIiNRiokBERERqceiBiIgMFoceNGNFgYiIiNRiRYGIiAwWZz1oxkSBiIgMFoceNOPQAxEREanFigIRERksFhQ0Y6JARESGi5mCRhx6ICIiIrVYUSAiIoPFWQ+aMVEgIiKDxVkPmnHogYiIiNRiRYGIiAwWCwqaMVEgIiLDxUxBIw49EBERkVqsKBARkcHirAfNmCgQEZHB4qwHzTj0QERERGpJBEEQxA6CXm5FRUWIjo5GZGQkZDKZ2OEQVQr+nJOhYqJALywvLw9WVlbIzc2FXC4XOxyiSsGfczJUHHogIiIitZgoEBERkVpMFIiIiEgtJgr0wmQyGWbMmMEHvKhG4885GSo+zEhERERqsaJAREREajFRICIiIrWYKBAREZFaTBRIhUQiwdatW8UOg6hS8eecSHtMFAxIVlYWxowZgyZNmkAmk6Fhw4bo1asX9u3bJ3ZoAABBEDB9+nQ4OjrCzMwMPj4+uHz5sthh0Uumuv+c//TTT/D19UWdOnUgkUiQkpIidkhEz8REwUBcu3YNbdu2RVJSEhYsWIDU1FTs2rULXbp0QVhYmNjhAQDmz5+PpUuXYuXKlTh27BjMzc3h5+eHwsJCsUOjl8TL8HP+4MEDdOjQAZ9//rnYoRBpRyCD0KNHD6F+/fpCfn5+hWP3799X/huAsGXLFuXnSZMmCW5uboKZmZng4uIiTJ06VSguLlYeT0lJETp37ixYWFgIlpaWQps2bYQTJ04IgiAI165dEwICAgRra2uhdu3agqenp5CQkPDU+MrKygQHBwdhwYIFyn05OTmCTCYTvv/++xe8ezIU1f3n/N8yMjIEAMLp06ef+36JqoKJyHkKVYF79+5h165dmDt3LszNzSsct7a2VnuupaUl4uLi4OTkhNTUVAwfPhyWlpaYNGkSAGDgwIF49dVXsWLFChgbGyMlJQW1atUCAISFhaG4uBiHDh2Cubk5Lly4AAsLi6deJyMjA1lZWfDx8VHus7KyQvv27ZGcnIygoKAX+AbIELwMP+dELyMmCgYgPT0dgiCgWbNmOp87depU5b8bN26MCRMm4IcfflD+BzQzMxMTJ05U9u3m5qZsn5mZicDAQHh5eQEAmjRpovY6WVlZAAB7e3uV/fb29spjRM/yMvycE72M+IyCARBeYPHNDRs2wNvbGw4ODrCwsMDUqVORmZmpPB4REYHQ0FD4+Phg3rx5uHLlivLYRx99hDlz5sDb2xszZszA2bNnX+g+iJ6FP+dElYOJggFwc3ODRCLBxYsXdTovOTkZAwcORM+ePbF9+3acPn0aU6ZMQXFxsbJNVFQUzp8/D39/fyQlJcHT0xNbtmwBAISGhuLq1asYNGgQUlNT0a5dOyxbtuyp13JwcAAAZGdnq+zPzs5WHiN6lpfh55zopSTuIxJUVbp3767zQ15ffPGF0KRJE5W2w4YNE6ysrNReJygoSOjVq9dTj33yySeCl5fXU4+VP8z4xRdfKPfl5ubyYUbSSXX/Of83PsxILwtWFAxEbGwsSktL8frrr2Pz5s24fPkyfv/9dyxduhQKheKp57i5uSEzMxM//PADrly5gqVLlyr/igKAgoIChIeH48CBA/jjjz/w66+/4sSJE/Dw8AAAjB07Frt370ZGRgZOnTqF/fv3K489SSKRYOzYsZgzZw5+/vlnpKamIjg4GE5OTujTp4/evw+qmar7zznw+KHLlJQUXLhwAQCQlpaGlJQUPotD1ZfYmQpVnZs3bwphYWGCs7OzIJVKhfr16wvvvPOOsH//fmUbPDFtbOLEiUKdOnUECwsLoX///kJMTIzyL62ioiIhKChIaNiwoSCVSgUnJychPDxcKCgoEARBEMLDw4WmTZsKMplMqFevnjBo0CDhr7/+UhtfWVmZMG3aNMHe3l6QyWRC165dhbS0tMr4KqgGq+4/52vWrBEAVNhmzJhRCd8G0Yvja6aJiIhILQ49EBERkVpMFIiIiEgtJgpERESkFhMFIiIiUouJAhEREanFRIGIiIjUYqJAREREajFRICIiIrWYKBDpweDBg1WWmu7cuTPGjh1b5XEcOHAAEokEOTk5lXaNJ+/1eVRFnESkH0wUqMYaPHgwJBIJJBIJpFIpXF1dMWvWLDx69KjSr/3TTz9h9uzZWrWt6l+ajRs3xuLFi6vkWkT08jMROwCiytS9e3esWbMGRUVF2LFjB8LCwlCrVi1ERkZWaFtcXAypVKqX69ra2uqlHyIisbGiQDWaTCaDg4MDnJ2dMXr0aPj4+ODnn38G8E8Jfe7cuXBycoK7uzsA4Pr163jvvfdgbW0NW1tb9O7dG9euXVP2WVpaioiICFhbW6NOnTqYNGkSnnxlypNDD0VFRZg8eTIaNmwImUwGV1dXfPPNN7h27Rq6dOkCALCxsYFEIsHgwYMBAGVlZYiOjoaLiwvMzMzQqlUr/PjjjyrX2bFjB1555RWYmZmhS5cuKnE+j9LSUgwbNkx5TXd3dyxZsuSpbWfOnIl69epBLpdj1KhRKC4uVh7TJvZ/++OPP9CrVy/Y2NjA3NwczZs3x44dO17oXohIP1hRIINiZmaGu3fvKj/v27cPcrkciYmJAICSkhL4+flBoVDgv//9L0xMTDBnzhx0794dZ8+ehVQqxcKFCxEXF4dvv/0WHh4eWLhwIbZs2YK3335b7XWDg4ORnJyMpUuXolWrVsjIyMBff/2Fhg0bYvPmzQgMDERaWhrkcjnMzMwAANHR0fjuu++wcuVKuLm54dChQ/jggw9Qr149dOrUCdevX0e/fv0QFhaGESNG4LfffsP48eNf6PspKytDgwYNsGnTJtSpUwdHjhzBiBEj4OjoiPfee0/lezM1NcWBAwdw7do1DBkyBHXq1MHcuXO1iv1JYWFhKC4uxqFDh2Bubo4LFy7AwsLihe6FiPRE5LdXElWakJAQoXfv3oIgPH6FdWJioiCTyYQJEyYoj9vb2wtFRUXKc9atWye4u7sLZWVlyn1FRUWCmZmZsHv3bkEQBMHR0VGYP3++8nhJSYnQoEED5bUEQRA6deokfPzxx4IgCEJaWpoAQEhMTHxqnPv37xcACPfv31fuKywsFGrXri0cOXJEpe2wYcOE999/XxAEQYiMjBQ8PT1Vjk+ePLlCX09ydnYWYmJi1B5/UlhYmBAYGKj8HBISItja2goPHjxQ7luxYoVgYWEhlJaWahX7k/fs5eUlREVFaR0TEVUdVhSoRtu+fTssLCxQUlKCsrIyDBgwAFFRUcrjXl5eKs8lnDlzBunp6bC0tFTpp7CwEFeuXEFubi5u3bqF9u3bK4+ZmJigXbt2FYYfyqWkpMDY2Pipf0mrk56ejocPH6Jbt24q+4uLi/Hqq68CAH7//XeVOABAoVBofQ11YmNj8e233yIzMxMFBQUoLi5G69atVdq0atUKtWvXVrlufn4+rl+/jvz8fI2xP+mjjz7C6NGjsWfPHvj4+CAwMBAtW7Z84XshohfHRIFqtC5dumDFihWQSqVwcnKCiYnqj7y5ubnK5/z8fLRt2xbr16+v0Fe9evWeK4byoQRd5OfnAwASEhJQv359lWMymey54tDGDz/8gAkTJmDhwoVQKBSwtLTEggULcOzYMa37eJ7YQ0ND4efnh4SEBOzZswfR0dFYuHAhxowZ8/w3Q0R6wUSBajRzc3O4urpq3b5NmzbYsGED7OzsIJfLn9rG0dERx44dQ8eOHQEAjx49wsmTJ9GmTZuntvfy8kJZWRkOHjwIHx+fCsfLKxqlpaXKfZ6enpDJZMjMzFRbifDw8FA+mFnu6NGjmm/yGX799Ve8+eab+PDDD5X7rly5UqHdmTNnUFBQoEyCjh49CgsLCzRs2BC2trYaY3+ahg0bYtSoURg1ahQiIyOxatUqJgpE1QBnPRD9y8CBA1G3bl307t0b//3vf5GRkYEDBw7go48+wp9//gkA+PjjjzFv3jxs3boVFy9exIcffvjMNRAaN26MkJAQDB06FFu3blX2uXHjRgCAs7MzJBIJtm/fjjt37iA/Px+WlpaYMGECxo0bh7Vr1+LKlSs4deoUli1bhrVr1wIARo0ahcuXL2PixIlIS0tDfHw84uLitLrPGzduICUlRWW7f/8+3Nzc8Ntvv2H37t24dOkSpk2bhhMnTlQ4v7i4GMOGDcOFCxewY8cOzJgxA+Hh4TAyMtIq9ieNHTsWu3fvRkZGBk6dOoX9+/fDw8NDq3shokom9kMSRJXl3w8z6nL81q1bQnBwsFC3bl1BJpMJTZo0EYYPHy7k5uYKgvD44cWPP/5YkMvlgrW1tRARESEEBwerfZhREAShoKBAGDdunODo6ChIpVLB1dVV+Pbbb5XHZ82aJTg4OAgSiUQICQkRBOHxA5iLFy8W3N3dhVq1agn16tUT/Pz8hIMHDyrP++WXXwRXV1dBJpMJb731lvDtt99q9TAjgArbunXrhMLCQmHw4MGClZWVYG1tLYwePVr45JNPhFatWlX43qZPny7UqVNHsLCwEIYPHy4UFhYq22iK/cmHGcPDw4WmTZsKMplMqFevnjBo0CDhr7/+UnsPRFR1JIKg5gksIiIiMngceiAiIiK1mCgQERGRWkwUiIiISC0mCkRERKQWEwUiIiJSi4kCERERqcVEgYiIiNRiokBERERqMVEgIiIitZgoEBERkVpMFIiIiEit/wcs6pMTTNrUrAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      4284\n",
      "           1       0.85      0.81      0.83      3983\n",
      "\n",
      "    accuracy                           0.84      8267\n",
      "   macro avg       0.84      0.84      0.84      8267\n",
      "weighted avg       0.84      0.84      0.84      8267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([8267, 2])\n",
      "y_test_tensor shape: torch.Size([8267, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions shape:\", predictions.shape)       # Before flattening\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)   # Before flattening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'model_complete.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
